{
  "model_id": "gemma-3n-e4b",
  "name": "Gemma 3n E4B",
  "organization_id": "google",
  "fine_tuned_from_model_id": null,
  "description": "Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",
  "release_date": "2025-06-26",
  "announcement_date": "2025-06-26",
  "license_id": "proprietary",
  "multimodal": true,
  "knowledge_cutoff": "2024-06-01",
  "param_count": 8000000000,
  "training_tokens": 11000000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://huggingface.co/blog/gemma3n",
  "source_playground": "https://aistudio.google.com/",
  "source_paper": null,
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_repo_link": null,
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E4B",
  "created_at": "2025-07-19T19:49:05.440084+00:00",
  "updated_at": "2025-07-19T19:49:05.440084+00:00",
  "model_family_id": null
}