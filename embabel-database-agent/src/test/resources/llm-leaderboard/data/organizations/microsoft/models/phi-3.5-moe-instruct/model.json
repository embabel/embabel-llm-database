{
  "model_id": "phi-3.5-moe-instruct",
  "name": "Phi-3.5-MoE-instruct",
  "organization_id": "microsoft",
  "fine_tuned_from_model_id": null,
  "description": "Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",
  "release_date": "2024-08-23",
  "announcement_date": "2024-08-23",
  "license_id": "mit",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 60000000000,
  "training_tokens": 4900000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
  "source_playground": null,
  "source_paper": "https://arxiv.org/abs/2404.14219",
  "source_scorecard_blog_link": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280",
  "source_repo_link": null,
  "source_weights_link": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
  "created_at": "2025-07-19T19:49:05.555819+00:00",
  "updated_at": "2025-07-19T19:49:05.555819+00:00",
  "model_family_id": null
}