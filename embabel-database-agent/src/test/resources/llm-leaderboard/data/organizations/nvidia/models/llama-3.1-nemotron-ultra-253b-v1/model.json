{
  "model_id": "llama-3.1-nemotron-ultra-253b-v1",
  "name": "Llama 3.1 Nemotron Ultra 253B v1",
  "organization_id": "nvidia",
  "fine_tuned_from_model_id": null,
  "description": "A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",
  "release_date": "2025-04-07",
  "announcement_date": "2025-04-07",
  "license_id": "llama_3_1_community_license",
  "multimodal": false,
  "knowledge_cutoff": "2023-12-01",
  "param_count": 253000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": null,
  "source_playground": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1",
  "source_paper": "https://arxiv.org/abs/2502.00203",
  "source_scorecard_blog_link": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard",
  "source_repo_link": null,
  "source_weights_link": "https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
  "created_at": "2025-07-19T19:49:05.735588+00:00",
  "updated_at": "2025-07-19T19:49:05.735588+00:00",
  "model_family_id": null
}