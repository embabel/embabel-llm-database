{
  "model_id": "kimi-k2-base",
  "name": "Kimi K2 Base",
  "organization_id": "moonshotai",
  "fine_tuned_from_model_id": null,
  "description": "Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",
  "release_date": "2025-01-01",
  "announcement_date": "2025-01-01",
  "license_id": "modified_mit_license",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 1000000000000,
  "training_tokens": 15500000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://platform.moonshot.ai",
  "source_playground": null,
  "source_paper": null,
  "source_scorecard_blog_link": "https://moonshotai.github.io/Kimi-K2/",
  "source_repo_link": "https://github.com/MoonshotAI/Kimi-K2",
  "source_weights_link": "https://huggingface.co/moonshotai/Kimi-K2-Base",
  "created_at": "2025-07-19T19:49:05.422399+00:00",
  "updated_at": "2025-07-19T19:49:05.422399+00:00",
  "model_family_id": null
}