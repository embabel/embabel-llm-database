[
  {
    "model_benchmark_id": 694,
    "benchmark_id": "aime-2025",
    "model_id": "grok-4-heavy",
    "score": 1.0,
    "normalized_score": 1.0,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.454500+00:00",
    "updated_at": "2025-07-19T19:56:12.454500+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 320,
    "benchmark_id": "gpqa",
    "model_id": "grok-4-heavy",
    "score": 0.884,
    "normalized_score": 0.884,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.708827+00:00",
    "updated_at": "2025-07-19T19:56:11.708827+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 1798,
    "benchmark_id": "hmmt25",
    "model_id": "grok-4-heavy",
    "score": 0.967,
    "normalized_score": 0.967,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.063588+00:00",
    "updated_at": "2025-07-19T19:56:15.063588+00:00",
    "benchmark_name": "HMMT25"
  },
  {
    "model_benchmark_id": 722,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "grok-4-heavy",
    "score": 0.507,
    "normalized_score": 0.507,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.521361+00:00",
    "updated_at": "2025-07-19T19:56:12.521361+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 1140,
    "benchmark_id": "livecodebench",
    "model_id": "grok-4-heavy",
    "score": 0.794,
    "normalized_score": 0.794,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.396669+00:00",
    "updated_at": "2025-07-19T19:56:13.396669+00:00",
    "benchmark_name": "LiveCodeBench"
  },
  {
    "model_benchmark_id": 1800,
    "benchmark_id": "usamo25",
    "model_id": "grok-4-heavy",
    "score": 0.619,
    "normalized_score": 0.619,
    "is_self_reported": true,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.070427+00:00",
    "updated_at": "2025-07-19T19:56:15.070427+00:00",
    "benchmark_name": "USAMO25"
  }
]
