[
  {
    "model_benchmark_id": 9002,
    "benchmark_id": "swe-bench-verified",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.749,
    "normalized_score": 0.749,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled (up to 128K tokens) with enhanced reasoning capabilities and iterative problem-solving approach.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "SWE-Bench Verified"
  },
  {
    "model_benchmark_id": 9004,
    "benchmark_id": "aider-polyglot",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.88,
    "normalized_score": 0.88,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled (up to 128K tokens) with step-by-step reasoning and multi-language code understanding.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Aider-Polyglot"
  },
  {
    "model_benchmark_id": 10027,
    "benchmark_id": "swe-lancer-(ic-diamond-subset)",
    "model_id": "gpt-5-2025-08-07",
    "score": 1.0,
    "normalized_score": 1.0,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - IC SWE Diamond Freelance Coding Tasks (earnings-based evaluation).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "SWE-Lancer (IC-Diamond subset)"
  },
  {
    "model_benchmark_id": 9020,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.946,
    "normalized_score": 0.946,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9009,
    "benchmark_id": "mmmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.842,
    "normalized_score": 0.842,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - College-level visual problem-solving with multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU"
  },
  {
    "model_benchmark_id": 9006,
    "benchmark_id": "mmlu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.925,
    "normalized_score": 0.925,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Standard benchmark across multiple academic subjects with comprehensive knowledge evaluation.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 9007,
    "benchmark_id": "humaneval",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.934,
    "normalized_score": 0.934,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Code generation benchmark with function completion tasks in Python.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 9008,
    "benchmark_id": "math",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.847,
    "normalized_score": 0.847,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled with step-by-step mathematical problem solving and verification.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 9013,
    "benchmark_id": "healthbench-hard",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.016,
    "normalized_score": 0.016,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled for medical hallucination detection. Measured inaccuracies on challenging healthcare conversations.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HealthBench Hard"
  },
  {
    "model_benchmark_id": 9024,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.263,
    "normalized_score": 0.263,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9028,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.933,
    "normalized_score": 0.933,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  },
  {
    "model_benchmark_id": 9032,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.857,
    "normalized_score": 0.857,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Diamond thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9037,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.248,
    "normalized_score": 0.248,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 standard with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9041,
    "benchmark_id": "scale-multichallenge",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.696,
    "normalized_score": 0.696,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Multi-turn instruction following benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Scale MultiChallenge"
  },
  {
    "model_benchmark_id": 9043,
    "benchmark_id": "browsecomp",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.549,
    "normalized_score": 0.549,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Agentic search & browsing benchmark.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "BrowseComp"
  },
  {
    "model_benchmark_id": 9045,
    "benchmark_id": "collie",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.99,
    "normalized_score": 0.99,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode enabled - Instruction-following in freeform writing.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "COLLIE"
  },
  {
    "model_benchmark_id": 10034,
    "benchmark_id": "multichallenge-(o3-mini-grader)",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.696,
    "normalized_score": 0.696,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with o3-mini grader - Multi-turn instruction following benchmark with improved grading accuracy.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MultiChallenge (o3-mini grader)"
  },
  {
    "model_benchmark_id": 10035,
    "benchmark_id": "internal-api-instruction-following-(hard)",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.64,
    "normalized_score": 0.64,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Internal API instruction following evaluation (hard difficulty).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Internal API instruction following (hard)"
  },
  {
    "model_benchmark_id": 9047,
    "benchmark_id": "tau2-airline",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.626,
    "normalized_score": 0.626,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 - Function calling benchmark (airline domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 airline"
  },
  {
    "model_benchmark_id": 9049,
    "benchmark_id": "tau2-retail",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.811,
    "normalized_score": 0.811,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Function calling benchmark (retail domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 retail"
  },
  {
    "model_benchmark_id": 9051,
    "benchmark_id": "tau2-telecom",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.967,
    "normalized_score": 0.967,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Function calling benchmark (telecom domain).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Tau2 telecom"
  },
  {
    "model_benchmark_id": 9053,
    "benchmark_id": "mmmu-pro",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.784,
    "normalized_score": 0.784,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "MMMU-Pro"
  },
  {
    "model_benchmark_id": 9055,
    "benchmark_id": "videommmu",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.846,
    "normalized_score": 0.846,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Video-based multimodal reasoning (max frame 256).",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "VideoMMMU"
  },
  {
    "model_benchmark_id": 9057,
    "benchmark_id": "charxiv-r",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.811,
    "normalized_score": 0.811,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Scientific figure reasoning and interpretation.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "CharXiv-R"
  },
  {
    "model_benchmark_id": 9059,
    "benchmark_id": "erqa",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.657,
    "normalized_score": 0.657,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 with thinking mode - Multimodal spatial reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "ERQA"
  },
  {
    "model_benchmark_id": 10048,
    "benchmark_id": "openai-mrcr:-2-needle-128k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.952,
    "normalized_score": 0.952,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "OpenAI-MRCR 2-needle retrieval at 128k tokens.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "OpenAI-MRCR: 2 needle 128k"
  },
  {
    "model_benchmark_id": 10049,
    "benchmark_id": "openai-mrcr:-2-needle-256k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.868,
    "normalized_score": 0.868,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "OpenAI-MRCR 2-needle retrieval at 256k tokens.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "OpenAI-MRCR: 2 needle 256k"
  },
  {
    "model_benchmark_id": 10050,
    "benchmark_id": "graphwalks-bfs-<128k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.783,
    "normalized_score": 0.783,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "Graphwalks BFS (<128k) long-context reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Graphwalks BFS <128k"
  },
  {
    "model_benchmark_id": 10051,
    "benchmark_id": "graphwalks-parents-<128k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.733,
    "normalized_score": 0.733,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "Graphwalks parents (<128k) long-context reasoning.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Graphwalks parents <128k"
  },
  {
    "model_benchmark_id": 10052,
    "benchmark_id": "browsecomp-long-128k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.9,
    "normalized_score": 0.9,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "BrowseComp long-context 128k variant.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "BrowseComp Long Context 128k"
  },
  {
    "model_benchmark_id": 10053,
    "benchmark_id": "browsecomp-long-256k",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.888,
    "normalized_score": 0.888,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "BrowseComp long-context 256k variant.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "BrowseComp Long Context 256k"
  },
  {
    "model_benchmark_id": 10054,
    "benchmark_id": "videomme-w-sub.",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.867,
    "normalized_score": 0.867,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "VideoMME (long) with subtitles category.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "VideoMME w sub."
  },
  {
    "model_benchmark_id": 10069,
    "benchmark_id": "longfact-concepts",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.007,
    "normalized_score": 0.007,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled for hallucination detection. Measured on open-source prompts for concept-based factual queries.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "LongFact-Concepts"
  },
  {
    "model_benchmark_id": 10070,
    "benchmark_id": "longfact-objects",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.008,
    "normalized_score": 0.008,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled for hallucination detection. Measured on open-source prompts for object-based factual queries.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "LongFact-Objects"
  },
  {
    "model_benchmark_id": 10071,
    "benchmark_id": "factscore",
    "model_id": "gpt-5-2025-08-07",
    "score": 0.01,
    "normalized_score": 0.01,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "verified_by_llmstats": false,
    "analysis_method": "Thinking mode enabled for factual accuracy assessment. Measured hallucination rate on open-source prompts.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FactScore"
  }
]
