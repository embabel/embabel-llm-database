[
  {
    "model_benchmark_id": 1917,
    "benchmark_id": "ai2-reasoning-challenge-(arc)",
    "model_id": "gpt-4-0613",
    "score": 0.963,
    "normalized_score": 0.963,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "25-shot, Grade-school multiple choice science questions (Challenge-set)",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.421959+00:00",
    "updated_at": "2025-07-19T19:56:15.421959+00:00",
    "benchmark_name": "AI2 Reasoning Challenge (ARC)"
  },
  {
    "model_benchmark_id": 965,
    "benchmark_id": "drop",
    "model_id": "gpt-4-0613",
    "score": 0.809,
    "normalized_score": 0.809,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "3-shot, Reading comprehension & arithmetic (f1 score)",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.028099+00:00",
    "updated_at": "2025-07-19T19:56:13.028099+00:00",
    "benchmark_name": "DROP"
  },
  {
    "model_benchmark_id": 362,
    "benchmark_id": "gpqa",
    "model_id": "gpt-4-0613",
    "score": 0.357,
    "normalized_score": 0.357,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/hello-gpt-4o/",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, Commonsense reasoning",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.775863+00:00",
    "updated_at": "2025-07-19T19:56:11.775863+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 55,
    "benchmark_id": "hellaswag",
    "model_id": "gpt-4-0613",
    "score": 0.953,
    "normalized_score": 0.953,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "10-shot, Commonsense reasoning around everyday events",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.199031+00:00",
    "updated_at": "2025-07-19T19:56:11.199031+00:00",
    "benchmark_name": "HellaSwag"
  },
  {
    "model_benchmark_id": 817,
    "benchmark_id": "humaneval",
    "model_id": "gpt-4-0613",
    "score": 0.67,
    "normalized_score": 0.67,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "0-shot, Python coding tasks",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.702020+00:00",
    "updated_at": "2025-07-19T19:56:12.702020+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 1915,
    "benchmark_id": "lsat",
    "model_id": "gpt-4-0613",
    "score": 0.88,
    "normalized_score": 0.88,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "Percentile score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.413295+00:00",
    "updated_at": "2025-07-19T19:56:15.413295+00:00",
    "benchmark_name": "LSAT"
  },
  {
    "model_benchmark_id": 432,
    "benchmark_id": "math",
    "model_id": "gpt-4-0613",
    "score": 0.42,
    "normalized_score": 0.42,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/hello-gpt-4o/",
    "verified_by_llmstats": false,
    "analysis_method": "Mathematics problem-solving",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.913379+00:00",
    "updated_at": "2025-07-19T19:56:11.913379+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 1302,
    "benchmark_id": "mgsm",
    "model_id": "gpt-4-0613",
    "score": 0.745,
    "normalized_score": 0.745,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/hello-gpt-4o/",
    "verified_by_llmstats": false,
    "analysis_method": "Mathematics problem-solving",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.721873+00:00",
    "updated_at": "2025-07-19T19:56:13.721873+00:00",
    "benchmark_name": "MGSM"
  },
  {
    "model_benchmark_id": 129,
    "benchmark_id": "mmlu",
    "model_id": "gpt-4-0613",
    "score": 0.864,
    "normalized_score": 0.864,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, Multiple-choice questions in 57 subjects (professional & academic)",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.336601+00:00",
    "updated_at": "2025-07-19T19:56:11.336601+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 1916,
    "benchmark_id": "sat-math",
    "model_id": "gpt-4-0613",
    "score": 0.89,
    "normalized_score": 0.89,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "Estimated from reported score of 710 out of 800",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.417889+00:00",
    "updated_at": "2025-07-19T19:56:15.417889+00:00",
    "benchmark_name": "SAT Math"
  },
  {
    "model_benchmark_id": 1914,
    "benchmark_id": "uniform-bar-exam",
    "model_id": "gpt-4-0613",
    "score": 0.9,
    "normalized_score": 0.9,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "Percentage score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.408427+00:00",
    "updated_at": "2025-07-19T19:56:15.408427+00:00",
    "benchmark_name": "Uniform Bar Exam"
  },
  {
    "model_benchmark_id": 156,
    "benchmark_id": "winogrande",
    "model_id": "gpt-4-0613",
    "score": 0.875,
    "normalized_score": 0.875,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot, Commonsense reasoning around pronoun resolution",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.396099+00:00",
    "updated_at": "2025-07-19T19:56:11.396099+00:00",
    "benchmark_name": "Winogrande"
  }
]