[
  {
    "model_benchmark_id": 9021,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-mini-2025-08-07",
    "score": 0.911,
    "normalized_score": 0.911,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 mini with thinking mode enabled (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9025,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-mini-2025-08-07",
    "score": 0.221,
    "normalized_score": 0.221,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 mini with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9033,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-mini-2025-08-07",
    "score": 0.823,
    "normalized_score": 0.823,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 mini - Diamond thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9038,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-mini-2025-08-07",
    "score": 0.167,
    "normalized_score": 0.167,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 mini with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9029,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-mini-2025-08-07",
    "score": 0.878,
    "normalized_score": 0.878,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 mini with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  }
]
