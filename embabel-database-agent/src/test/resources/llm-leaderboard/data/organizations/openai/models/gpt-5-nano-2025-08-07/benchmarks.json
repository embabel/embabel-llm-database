[
  {
    "model_benchmark_id": 9022,
    "benchmark_id": "aime-2025",
    "model_id": "gpt-5-nano-2025-08-07",
    "score": 0.852,
    "normalized_score": 0.852,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 nano with thinking mode enabled (no tools) - competition mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "AIME 2025"
  },
  {
    "model_benchmark_id": 9026,
    "benchmark_id": "frontiermath",
    "model_id": "gpt-5-nano-2025-08-07",
    "score": 0.096,
    "normalized_score": 0.096,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 nano with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "FrontierMath"
  },
  {
    "model_benchmark_id": 9034,
    "benchmark_id": "gpqa",
    "model_id": "gpt-5-nano-2025-08-07",
    "score": 0.712,
    "normalized_score": 0.712,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 nano - Diamond thinking no tools",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 9039,
    "benchmark_id": "humanity's-last-exam",
    "model_id": "gpt-5-nano-2025-08-07",
    "score": 0.087,
    "normalized_score": 0.087,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 nano with thinking mode (no tools) - Full set of expert-level questions across subjects.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "Humanity's Last Exam"
  },
  {
    "model_benchmark_id": 9030,
    "benchmark_id": "hmmt-2025",
    "model_id": "gpt-5-nano-2025-08-07",
    "score": 0.756,
    "normalized_score": 0.756,
    "is_self_reported": true,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "verified_by_llmstats": false,
    "analysis_method": "GPT-5 nano with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "benchmark_name": "HMMT 2025"
  }
]
