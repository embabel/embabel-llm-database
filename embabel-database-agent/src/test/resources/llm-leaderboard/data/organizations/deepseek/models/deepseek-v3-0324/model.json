{
  "model_id": "deepseek-v3-0324",
  "name": "DeepSeek-V3 0324",
  "organization_id": "deepseek",
  "fine_tuned_from_model_id": null,
  "description": "A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",
  "release_date": "2025-03-25",
  "announcement_date": "2025-03-25",
  "license_id": "mit_+_model_license_(commercial_use_allowed)",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 671000000000,
  "training_tokens": 14800000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://platform.deepseek.com",
  "source_playground": "https://chat.deepseek.com",
  "source_paper": "https://arxiv.org/abs/2412.19437",
  "source_scorecard_blog_link": null,
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-V3",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
  "created_at": "2025-07-19T19:49:05.693499+00:00",
  "updated_at": "2025-07-19T19:49:05.693499+00:00",
  "model_family_id": null
}