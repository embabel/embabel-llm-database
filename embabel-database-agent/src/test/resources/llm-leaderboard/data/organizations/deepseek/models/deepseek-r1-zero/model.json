{
  "model_id": "deepseek-r1-zero",
  "name": "DeepSeek R1 Zero",
  "organization_id": "deepseek",
  "fine_tuned_from_model_id": "deepseek-v3",
  "description": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",
  "release_date": "2025-01-20",
  "announcement_date": "2025-01-20",
  "license_id": "mit",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 671000000000,
  "training_tokens": 14800000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_playground": "https://chat.deepseek.com",
  "source_paper": "https://arxiv.org/abs/2501.12948",
  "source_scorecard_blog_link": null,
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
  "created_at": "2025-07-19T19:49:05.902496+00:00",
  "updated_at": "2025-07-19T19:49:05.902496+00:00",
  "model_family_id": null
}