{
  "model_id": "qwen3-30b-a3b",
  "name": "Qwen3 30B A3B",
  "organization_id": "qwen",
  "fine_tuned_from_model_id": null,
  "description": "Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",
  "release_date": "2025-04-29",
  "announcement_date": "2025-04-29",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 30500000000,
  "training_tokens": 36000000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://qwenlm.github.io/blog/qwen3/",
  "source_playground": "https://chat.qwen.ai/",
  "source_paper": null,
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
  "created_at": "2025-07-19T19:49:05.631206+00:00",
  "updated_at": "2025-07-19T19:49:05.631206+00:00",
  "model_family_id": null
}