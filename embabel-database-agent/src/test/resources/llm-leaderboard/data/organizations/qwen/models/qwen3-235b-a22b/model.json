{
  "model_id": "qwen3-235b-a22b",
  "name": "Qwen3 235B A22B",
  "organization_id": "qwen",
  "fine_tuned_from_model_id": null,
  "description": "Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",
  "release_date": "2025-04-29",
  "announcement_date": "2025-04-29",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": null,
  "param_count": 235000000000,
  "training_tokens": 36000000000000,
  "available_in_zeroeval": true,
  "source_api_ref": "https://qwenlm.github.io/blog/qwen3/",
  "source_playground": "https://chat.qwen.ai/",
  "source_paper": null,
  "source_scorecard_blog_link": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B",
  "created_at": "2025-07-19T19:49:05.624683+00:00",
  "updated_at": "2025-07-19T19:49:05.624683+00:00",
  "model_family_id": null
}