{
  "model_id": "mistral-small-24b-instruct-2501",
  "name": "Mistral Small 3 24B Instruct",
  "organization_id": "mistral",
  "fine_tuned_from_model_id": null,
  "description": "Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2\u20133\u00d7 its size while using significantly fewer compute resources.",
  "release_date": "2025-01-30",
  "announcement_date": "2025-01-30",
  "license_id": "apache_2_0",
  "multimodal": false,
  "knowledge_cutoff": "2023-10-01",
  "param_count": 24000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_playground": null,
  "source_paper": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3/",
  "source_repo_link": null,
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
  "created_at": "2025-07-19T19:49:05.788628+00:00",
  "updated_at": "2025-07-19T19:49:05.788628+00:00",
  "model_family_id": null
}