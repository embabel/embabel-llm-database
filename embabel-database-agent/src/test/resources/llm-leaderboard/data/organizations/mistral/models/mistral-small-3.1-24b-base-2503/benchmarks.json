[
  {
    "model_benchmark_id": 346,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-3.1-24b-base-2503",
    "score": 0.375,
    "normalized_score": 0.375,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "verified_by_llmstats": false,
    "analysis_method": "accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.749533+00:00",
    "updated_at": "2025-07-19T19:56:11.749533+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 114,
    "benchmark_id": "mmlu",
    "model_id": "mistral-small-3.1-24b-base-2503",
    "score": 0.8101,
    "normalized_score": 0.8101,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.312907+00:00",
    "updated_at": "2025-07-19T19:56:11.312907+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 218,
    "benchmark_id": "mmlu-pro",
    "model_id": "mistral-small-3.1-24b-base-2503",
    "score": 0.5603,
    "normalized_score": 0.5603,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "verified_by_llmstats": false,
    "analysis_method": "0-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.513719+00:00",
    "updated_at": "2025-07-19T19:56:11.513719+00:00",
    "benchmark_name": "MMLU-Pro"
  },
  {
    "model_benchmark_id": 587,
    "benchmark_id": "mmmu",
    "model_id": "mistral-small-3.1-24b-base-2503",
    "score": 0.5927,
    "normalized_score": 0.5927,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "verified_by_llmstats": false,
    "analysis_method": "CoT accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.207080+00:00",
    "updated_at": "2025-07-19T19:56:12.207080+00:00",
    "benchmark_name": "MMMU"
  },
  {
    "model_benchmark_id": 255,
    "benchmark_id": "triviaqa",
    "model_id": "mistral-small-3.1-24b-base-2503",
    "score": 0.805,
    "normalized_score": 0.805,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.587622+00:00",
    "updated_at": "2025-07-19T19:56:11.587622+00:00",
    "benchmark_name": "TriviaQA"
  }
]
