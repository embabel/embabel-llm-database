[
  {
    "model_benchmark_id": 1014,
    "benchmark_id": "gsm8k",
    "model_id": "mistral-large-2-2407",
    "score": 0.93,
    "normalized_score": 0.93,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "verified_by_llmstats": false,
    "analysis_method": "Accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.113392+00:00",
    "updated_at": "2025-07-19T19:56:13.113392+00:00",
    "benchmark_name": "GSM8k"
  },
  {
    "model_benchmark_id": 810,
    "benchmark_id": "humaneval",
    "model_id": "mistral-large-2-2407",
    "score": 0.92,
    "normalized_score": 0.92,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "verified_by_llmstats": false,
    "analysis_method": "Pass@1",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.687406+00:00",
    "updated_at": "2025-07-19T19:56:12.687406+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 116,
    "benchmark_id": "mmlu",
    "model_id": "mistral-large-2-2407",
    "score": 0.84,
    "normalized_score": 0.84,
    "is_self_reported": true,
    "self_reported_source_link": "https://mistral.ai/news/mistral-large-2407/",
    "verified_by_llmstats": false,
    "analysis_method": "Accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.316024+00:00",
    "updated_at": "2025-07-19T19:56:11.316024+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 1828,
    "benchmark_id": "mmlu-french",
    "model_id": "mistral-large-2-2407",
    "score": 0.828,
    "normalized_score": 0.828,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "verified_by_llmstats": false,
    "analysis_method": "Accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.178056+00:00",
    "updated_at": "2025-07-19T19:56:15.178056+00:00",
    "benchmark_name": "MMLU French"
  },
  {
    "model_benchmark_id": 1615,
    "benchmark_id": "mt-bench",
    "model_id": "mistral-large-2-2407",
    "score": 86.3,
    "normalized_score": 0.863,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:14.541051+00:00",
    "updated_at": "2025-07-19T19:56:14.541051+00:00",
    "benchmark_name": "MT-Bench"
  }
]