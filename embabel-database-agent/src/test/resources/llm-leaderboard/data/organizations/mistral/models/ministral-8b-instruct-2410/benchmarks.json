[
  {
    "model_benchmark_id": 1410,
    "benchmark_id": "agieval",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.483,
    "normalized_score": 0.483,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.978647+00:00",
    "updated_at": "2025-07-19T19:56:13.978647+00:00",
    "benchmark_name": "AGIEval"
  },
  {
    "model_benchmark_id": 30,
    "benchmark_id": "arc-c",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.719,
    "normalized_score": 0.719,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.142536+00:00",
    "updated_at": "2025-07-19T19:56:11.142536+00:00",
    "benchmark_name": "ARC-C"
  },
  {
    "model_benchmark_id": 1464,
    "benchmark_id": "arena-hard",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.709,
    "normalized_score": 0.709,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:14.118772+00:00",
    "updated_at": "2025-07-19T19:56:14.118772+00:00",
    "benchmark_name": "Arena Hard"
  },
  {
    "model_benchmark_id": 1820,
    "benchmark_id": "french-mmlu",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.575,
    "normalized_score": 0.575,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.137792+00:00",
    "updated_at": "2025-07-19T19:56:15.137792+00:00",
    "benchmark_name": "French MMLU"
  },
  {
    "model_benchmark_id": 806,
    "benchmark_id": "humaneval",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.348,
    "normalized_score": 0.348,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.681246+00:00",
    "updated_at": "2025-07-19T19:56:12.681246+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 422,
    "benchmark_id": "math",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.545,
    "normalized_score": 0.545,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.895272+00:00",
    "updated_at": "2025-07-19T19:56:11.895272+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 1821,
    "benchmark_id": "mbpp-pass@1",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.7,
    "normalized_score": 0.7,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:15.141858+00:00",
    "updated_at": "2025-07-19T19:56:15.141858+00:00",
    "benchmark_name": "MBPP pass@1"
  },
  {
    "model_benchmark_id": 112,
    "benchmark_id": "mmlu",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.65,
    "normalized_score": 0.65,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.309619+00:00",
    "updated_at": "2025-07-19T19:56:11.309619+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 1612,
    "benchmark_id": "mt-bench",
    "model_id": "ministral-8b-instruct-2410",
    "score": 83.0,
    "normalized_score": 0.83,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "Score",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:14.535003+00:00",
    "updated_at": "2025-07-19T19:56:14.535003+00:00",
    "benchmark_name": "MT-Bench"
  },
  {
    "model_benchmark_id": 253,
    "benchmark_id": "triviaqa",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.655,
    "normalized_score": 0.655,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.582765+00:00",
    "updated_at": "2025-07-19T19:56:11.582765+00:00",
    "benchmark_name": "TriviaQA"
  },
  {
    "model_benchmark_id": 155,
    "benchmark_id": "winogrande",
    "model_id": "ministral-8b-instruct-2410",
    "score": 0.753,
    "normalized_score": 0.753,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.394106+00:00",
    "updated_at": "2025-07-19T19:56:11.394106+00:00",
    "benchmark_name": "Winogrande"
  }
]
