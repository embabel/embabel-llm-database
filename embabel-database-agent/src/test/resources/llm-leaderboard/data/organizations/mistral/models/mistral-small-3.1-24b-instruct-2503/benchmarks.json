[
  {
    "model_benchmark_id": 340,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.4596,
    "normalized_score": 0.4596,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "Diamond, 5-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.740584+00:00",
    "updated_at": "2025-07-19T19:56:11.741944+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 805,
    "benchmark_id": "humaneval",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.8841,
    "normalized_score": 0.8841,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "standard",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.677771+00:00",
    "updated_at": "2025-07-19T19:56:12.677771+00:00",
    "benchmark_name": "HumanEval"
  },
  {
    "model_benchmark_id": 421,
    "benchmark_id": "math",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.693,
    "normalized_score": 0.693,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "standard",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.893255+00:00",
    "updated_at": "2025-07-19T19:56:11.893255+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 1194,
    "benchmark_id": "mbpp",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 74.71,
    "normalized_score": 0.7471,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "standard",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:13.514872+00:00",
    "updated_at": "2025-07-19T19:56:13.514872+00:00",
    "benchmark_name": "MBPP"
  },
  {
    "model_benchmark_id": 110,
    "benchmark_id": "mmlu",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.8062,
    "normalized_score": 0.8062,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.306426+00:00",
    "updated_at": "2025-07-19T19:56:11.306426+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 215,
    "benchmark_id": "mmlu-pro",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.6676,
    "normalized_score": 0.6676,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.508555+00:00",
    "updated_at": "2025-07-19T19:56:11.508555+00:00",
    "benchmark_name": "MMLU-Pro"
  },
  {
    "model_benchmark_id": 585,
    "benchmark_id": "mmmu",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.5927,
    "normalized_score": 0.5927,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "CoT accuracy",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:12.203401+00:00",
    "updated_at": "2025-07-19T19:56:12.203401+00:00",
    "benchmark_name": "MMMU"
  },
  {
    "model_benchmark_id": 237,
    "benchmark_id": "simpleqa",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.1043,
    "normalized_score": 0.1043,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "TotalAcc, Correct",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.552923+00:00",
    "updated_at": "2025-07-19T19:56:11.552923+00:00",
    "benchmark_name": "SimpleQA"
  },
  {
    "model_benchmark_id": 251,
    "benchmark_id": "triviaqa",
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "score": 0.805,
    "normalized_score": 0.805,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-07-19T19:56:11.579482+00:00",
    "updated_at": "2025-07-19T19:56:11.579482+00:00",
    "benchmark_name": "TriviaQA"
  }
]
