[
  {
    "model_benchmark_id": 16767,
    "benchmark_id": "ai2d",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.9291,
    "normalized_score": 0.9291,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.105841+00:00",
    "updated_at": "2025-08-03T22:06:15.105841+00:00",
    "benchmark_name": "AI2D"
  },
  {
    "model_benchmark_id": 16768,
    "benchmark_id": "arena-hard",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.431,
    "normalized_score": 0.431,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "v2",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.107885+00:00",
    "updated_at": "2025-08-03T22:06:15.107885+00:00",
    "benchmark_name": "Arena Hard"
  },
  {
    "model_benchmark_id": 16769,
    "benchmark_id": "chartqa",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.874,
    "normalized_score": 0.874,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.109760+00:00",
    "updated_at": "2025-08-03T22:06:15.109760+00:00",
    "benchmark_name": "ChartQA"
  },
  {
    "model_benchmark_id": 16770,
    "benchmark_id": "docvqa",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.9486,
    "normalized_score": 0.9486,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.111977+00:00",
    "updated_at": "2025-08-03T22:06:15.111977+00:00",
    "benchmark_name": "DocVQA"
  },
  {
    "model_benchmark_id": 16771,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.4422,
    "normalized_score": 0.4422,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.113518+00:00",
    "updated_at": "2025-08-03T22:06:15.113518+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 16772,
    "benchmark_id": "gpqa",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.4613,
    "normalized_score": 0.4613,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.115179+00:00",
    "updated_at": "2025-08-03T22:06:15.115179+00:00",
    "benchmark_name": "GPQA"
  },
  {
    "model_benchmark_id": 16773,
    "benchmark_id": "humaneval-plus",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.929,
    "normalized_score": 0.929,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "Pass@5",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.116763+00:00",
    "updated_at": "2025-08-03T22:06:15.116763+00:00",
    "benchmark_name": "HumanEval Plus"
  },
  {
    "model_benchmark_id": 16774,
    "benchmark_id": "if",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.8478,
    "normalized_score": 0.8478,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.118250+00:00",
    "updated_at": "2025-08-03T22:06:15.118250+00:00",
    "benchmark_name": "IF"
  },
  {
    "model_benchmark_id": 16775,
    "benchmark_id": "math",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.6942,
    "normalized_score": 0.6942,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.119723+00:00",
    "updated_at": "2025-08-03T22:06:15.119723+00:00",
    "benchmark_name": "MATH"
  },
  {
    "model_benchmark_id": 16776,
    "benchmark_id": "mathvista",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.6709,
    "normalized_score": 0.6709,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.121246+00:00",
    "updated_at": "2025-08-03T22:06:15.121246+00:00",
    "benchmark_name": "MathVista"
  },
  {
    "model_benchmark_id": 16777,
    "benchmark_id": "mbpp-plus",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.7833,
    "normalized_score": 0.7833,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "Pass@5",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.122828+00:00",
    "updated_at": "2025-08-03T22:06:15.122828+00:00",
    "benchmark_name": "MBPP Plus"
  },
  {
    "model_benchmark_id": 16778,
    "benchmark_id": "mmlu",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.805,
    "normalized_score": 0.805,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.124220+00:00",
    "updated_at": "2025-08-03T22:06:15.124220+00:00",
    "benchmark_name": "MMLU"
  },
  {
    "model_benchmark_id": 16779,
    "benchmark_id": "mmlu-pro",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.6906,
    "normalized_score": 0.6906,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "5-shot CoT",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.125972+00:00",
    "updated_at": "2025-08-03T22:06:15.125972+00:00",
    "benchmark_name": "MMLU-Pro"
  },
  {
    "model_benchmark_id": 16780,
    "benchmark_id": "mmmu",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.625,
    "normalized_score": 0.625,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "-",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.127425+00:00",
    "updated_at": "2025-08-03T22:06:15.127425+00:00",
    "benchmark_name": "MMMU"
  },
  {
    "model_benchmark_id": 16781,
    "benchmark_id": "simpleqa",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.121,
    "normalized_score": 0.121,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "TotalAcc",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.129114+00:00",
    "updated_at": "2025-08-03T22:06:15.129114+00:00",
    "benchmark_name": "SimpleQA"
  },
  {
    "model_benchmark_id": 16782,
    "benchmark_id": "wild-bench",
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "score": 0.6533,
    "normalized_score": 0.6533,
    "is_self_reported": true,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "verified_by_llmstats": false,
    "analysis_method": "v2",
    "verification_provider_id": null,
    "verification_hardware": null,
    "verification_date": null,
    "verification_notes": null,
    "created_at": "2025-08-03T22:06:15.130665+00:00",
    "updated_at": "2025-08-03T22:06:15.130665+00:00",
    "benchmark_name": "Wild Bench"
  }
]
