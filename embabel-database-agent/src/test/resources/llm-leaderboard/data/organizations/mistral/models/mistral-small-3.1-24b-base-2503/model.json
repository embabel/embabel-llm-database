{
  "model_id": "mistral-small-3.1-24b-base-2503",
  "name": "Mistral Small 3.1 24B Base",
  "organization_id": "mistral",
  "fine_tuned_from_model_id": null,
  "description": "Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",
  "release_date": "2025-03-17",
  "announcement_date": "2025-03-17",
  "license_id": "apache_2_0",
  "multimodal": true,
  "knowledge_cutoff": null,
  "param_count": 24000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "source_api_ref": null,
  "source_playground": "https://console.mistral.ai/",
  "source_paper": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3-1",
  "source_repo_link": null,
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
  "created_at": "2025-07-19T19:49:05.793911+00:00",
  "updated_at": "2025-07-19T19:49:05.793911+00:00",
  "model_family_id": null
}