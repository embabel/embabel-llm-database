{
  "model_id": "qwen3-vl-32b-instruct",
  "name": "Qwen3 VL 32B Instruct",
  "organization": {
    "id": "qwen",
    "name": "Alibaba Cloud / Qwen Team",
    "website": "https://qwenlm.github.io"
  },
  "description": "Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The “Instruct” version rivals Gemini 2.5 Pro in perception benchmarks, while the “Thinking” version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence.",
  "release_date": "2025-09-22",
  "announcement_date": "2025-09-22",
  "multimodal": true,
  "knowledge_cutoff": null,
  "param_count": 33000000000,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "license": {
    "name": "Apache 2.0",
    "allow_commercial": true
  },
  "model_family": null,
  "fine_tuned_from": null,
  "tags": {
    "tuning": "instruct"
  },
  "sources": {
    "api_ref": "https://help.aliyun.com/zh/model-studio/use-qwen-by-calling-api",
    "playground": "https://chat.qwen.ai/",
    "paper": null,
    "scorecard_blog": "https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list",
    "repo": "https://github.com/QwenLM/Qwen3-VL",
    "weights": "https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"
  },
  "providers": []
}
