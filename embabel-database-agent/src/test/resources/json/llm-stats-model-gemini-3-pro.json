{
  "model_id": "gemini-3-pro-preview",
  "name": "Gemini 3 Pro",
  "organization": {
    "id": "google",
    "name": "Google",
    "website": "https://google.com"
  },
  "description": "Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking by default to reason through prompts, and features a 1 million-token input context window with 64k output tokens.",
  "release_date": "2025-11-18",
  "announcement_date": "2025-11-18",
  "multimodal": true,
  "knowledge_cutoff": "2025-01-31",
  "param_count": null,
  "training_tokens": null,
  "available_in_zeroeval": true,
  "license": {
    "name": "Proprietary",
    "allow_commercial": false
  },
  "model_family": null,
  "fine_tuned_from": null,
  "tags": null,
  "sources": {
    "api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini-3-pro",
    "playground": "https://aistudio.google.com/",
    "paper": null,
    "scorecard_blog": "https://blog.google/products/gemini/gemini-3",
    "repo": null,
    "weights": null
  },
  "benchmarks": [
    {
      "benchmark_id": "aime-2025",
      "name": "AIME 2025",
      "description": "All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",
      "categories": [
        "math",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 1.0,
      "normalized_score": 1.0,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "With code execution",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "arc-agi-v2",
      "name": "ARC-AGI v2",
      "description": "ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",
      "categories": [
        "reasoning",
        "spatial_reasoning",
        "vision"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.311,
      "normalized_score": 0.311,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "ARC Prize Verified",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "charxiv-r",
      "name": "CharXiv-R",
      "description": "CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",
      "categories": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.814,
      "normalized_score": 0.814,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "facts-grounding",
      "name": "FACTS Grounding",
      "description": "A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",
      "categories": [
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.705,
      "normalized_score": 0.705,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": "Held out internal grounding, parametric, MM, and search retrieval benchmarks"
    },
    {
      "benchmark_id": "global-piqa",
      "name": "Global PIQA",
      "description": "Global PIQA is a multilingual commonsense reasoning benchmark that evaluates physical interaction knowledge across 100 languages and cultures. It tests AI systems' understanding of physical world knowledge in diverse cultural contexts through multiple choice questions about everyday situations requiring physical commonsense.",
      "categories": [
        "general",
        "physics",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.934,
      "normalized_score": 0.934,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": "Commonsense reasoning across 100 Languages and Cultures"
    },
    {
      "benchmark_id": "gpqa",
      "name": "GPQA",
      "description": "A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",
      "categories": [
        "general",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.919,
      "normalized_score": 0.919,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "No tools",
      "verification_date": null,
      "verification_notes": "GPQA Diamond subset"
    },
    {
      "benchmark_id": "humanity's-last-exam",
      "name": "Humanity's Last Exam",
      "description": "A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",
      "categories": [
        "general"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.458,
      "normalized_score": 0.458,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "With search and code execution",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "livecodebench-pro",
      "name": "LiveCodeBench Pro",
      "description": "LiveCodeBench Pro is an advanced evaluation benchmark for large language models for code that uses Elo ratings to rank models based on their performance on coding tasks. It evaluates models on real-world coding problems from programming contests (LeetCode, AtCoder, CodeForces) and provides a relative ranking system where higher Elo scores indicate superior performance.",
      "categories": [
        "code",
        "general",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 3000.0,
      "score": 2439.0,
      "normalized_score": 0.813,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "Elo Rating",
      "verification_date": null,
      "verification_notes": "Higher is better"
    },
    {
      "benchmark_id": "matharena-apex",
      "name": "MathArena Apex",
      "description": "MathArena Apex is a challenging math contest benchmark featuring the most difficult mathematical problems designed to test advanced reasoning and problem-solving abilities of AI models. It focuses on olympiad-level mathematics and complex multi-step mathematical reasoning.",
      "categories": [
        "math",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.234,
      "normalized_score": 0.234,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "mmmlu",
      "name": "MMMLU",
      "description": "Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",
      "categories": [
        "general",
        "language",
        "math",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.918,
      "normalized_score": 0.918,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "mmmu-pro",
      "name": "MMMU-Pro",
      "description": "A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",
      "categories": [
        "general",
        "multimodal",
        "reasoning",
        "vision"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.81,
      "normalized_score": 0.81,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "mrcr-v2-(8-needle)",
      "name": "MRCR v2 (8-needle)",
      "description": "MRCR v2 (8-needle) is a variant of the Multi-Round Coreference Resolution benchmark that includes 8 needle items to retrieve from long contexts. This tests models' ability to simultaneously track and reason about multiple pieces of information across extended conversations.",
      "categories": [
        "general",
        "long_context",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.263,
      "normalized_score": 0.263,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "1M (pointwise)",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "omnidocbench-1.5",
      "name": "OmniDocBench 1.5",
      "description": "OmniDocBench 1.5 is a comprehensive benchmark for evaluating multimodal large language models on document understanding tasks, including OCR, document parsing, information extraction, and visual question answering across diverse document types. Lower Overall Edit Distance scores are better.",
      "categories": [
        "multimodal",
        "reasoning",
        "vision",
        "structured_output"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.115,
      "normalized_score": null,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "Overall Edit Distance",
      "verification_date": null,
      "verification_notes": "Lower is better"
    },
    {
      "benchmark_id": "screenspot-pro",
      "name": "ScreenSpot Pro",
      "description": "ScreenSpot-Pro is a novel GUI grounding benchmark designed to rigorously evaluate the grounding capabilities of multimodal large language models (MLLMs) in professional high-resolution computing environments. The benchmark comprises 1,581 instructions across 23 applications spanning 5 industries and 3 operating systems, featuring authentic high-resolution images from professional domains with expert annotations. Unlike previous benchmarks that focus on cropped screenshots in consumer applications, ScreenSpot-Pro addresses the complexity and diversity of real-world professional software scenarios, revealing significant performance gaps in current MLLM GUI perception capabilities.",
      "categories": [
        "multimodal",
        "spatial_reasoning",
        "vision"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.727,
      "normalized_score": 0.727,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "simpleqa",
      "name": "SimpleQA",
      "description": "SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",
      "categories": [
        "general",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.721,
      "normalized_score": 0.721,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "swe-bench-verified",
      "name": "SWE-Bench Verified",
      "description": "A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",
      "categories": [
        "code",
        "frontend_development",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.762,
      "normalized_score": 0.762,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "Single attempt",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "t2-bench",
      "name": "t2-bench",
      "description": "t2-bench is a benchmark for evaluating agentic tool use capabilities, measuring how well models can select, sequence, and utilize tools to solve complex tasks. It tests autonomous planning and execution in multi-step scenarios.",
      "categories": [
        "agents",
        "reasoning",
        "tool_calling"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.854,
      "normalized_score": 0.854,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "terminal-bench-2",
      "name": "Terminal-Bench 2.0",
      "description": "Terminal-Bench 2.0 is an updated benchmark for testing AI agents' tool use ability to operate a computer via terminal. It evaluates how well models can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities.",
      "categories": [
        "agents",
        "code",
        "reasoning",
        "tool_calling"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 0.542,
      "normalized_score": 0.542,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "Terminus-2 agent",
      "verification_date": null,
      "verification_notes": null
    },
    {
      "benchmark_id": "vending-bench-2",
      "name": "Vending-Bench 2",
      "description": "Vending-Bench 2 tests longer horizon planning capabilities by evaluating how well AI models can manage a simulated vending machine business over extended periods. The benchmark measures a model's ability to maintain consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task.",
      "categories": [
        "agents",
        "reasoning"
      ],
      "modality": "text",
      "max_score": 1.0,
      "score": 5478.16,
      "normalized_score": null,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": "Net worth (mean)",
      "verification_date": null,
      "verification_notes": "Higher is better"
    },
    {
      "benchmark_id": "videommmu",
      "name": "VideoMMMU",
      "description": "Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",
      "categories": [
        "healthcare",
        "multimodal",
        "reasoning",
        "vision"
      ],
      "modality": "multimodal",
      "max_score": 1.0,
      "score": 0.876,
      "normalized_score": 0.876,
      "verified": false,
      "self_reported": true,
      "self_reported_source": "https://blog.google/products/gemini/gemini-3",
      "analysis_method": null,
      "verification_date": null,
      "verification_notes": null
    }
  ],
  "providers": [
    {
      "provider_id": "google",
      "name": "Google",
      "website": "https://ai.google.dev",
      "deprecated": false,
      "deprecated_at": null,
      "pricing": {
        "input_per_million": 2.0,
        "output_per_million": 12.0
      },
      "quantization": null,
      "limits": {
        "max_input_tokens": 1048576,
        "max_output_tokens": 65536
      },
      "performance": {
        "throughput": "90.0",
        "latency": "0.6"
      },
      "features": {
        "web_search": null,
        "function_calling": null,
        "structured_output": null,
        "code_execution": null,
        "batch_inference": null,
        "finetuning": null
      },
      "modalities": {
        "input": {
          "text": true,
          "image": true,
          "audio": true,
          "video": true
        },
        "output": {
          "text": true,
          "image": false,
          "audio": false,
          "video": false
        }
      }
    }
  ],
  "benchmark_rankings": [
    {
      "benchmark_id": "aime-2025",
      "benchmark_name": "AIME 2025",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 1.0,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "grok-4-heavy",
          "model_name": "Grok-4 Heavy",
          "score": 1.0,
          "rank": 1,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.946,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-2025-11-13",
          "model_name": "GPT-5.1",
          "score": 0.94,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-instant-2025-11-12",
          "model_name": "GPT-5.1 Instant",
          "score": 0.94,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-thinking-2025-11-12",
          "model_name": "GPT-5.1 Thinking",
          "score": 0.94,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "glm-4.6",
          "model_name": "GLM-4.6",
          "score": 0.939,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "grok-3",
          "model_name": "Grok-3",
          "score": 0.933,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "o4-mini",
          "model_name": "o4-mini",
          "score": 0.927,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-235b-a22b-thinking-2507",
          "model_name": "Qwen3-235B-A22B-Thinking-2507",
          "score": 0.923,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "charxiv-r",
      "benchmark_name": "CharXiv-R",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.814,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.811,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "o3-2025-04-16",
          "model_name": "o3",
          "score": 0.786,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "o4-mini",
          "model_name": "o4-mini",
          "score": 0.72,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-thinking",
          "model_name": "Qwen3 VL 235B A22B Thinking",
          "score": 0.661,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-thinking",
          "model_name": "Qwen3 VL 32B Thinking",
          "score": 0.652,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-instruct",
          "model_name": "Qwen3 VL 32B Instruct",
          "score": 0.628,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-instruct",
          "model_name": "Qwen3 VL 235B A22B Instruct",
          "score": 0.621,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "gpt-4o-2024-08-06",
          "model_name": "GPT-4o",
          "score": 0.588,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "gpt-4.1-mini-2025-04-14",
          "model_name": "GPT-4.1 mini",
          "score": 0.568,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "gpqa",
      "benchmark_name": "GPQA",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.919,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "grok-4-heavy",
          "model_name": "Grok-4 Heavy",
          "score": 0.884,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-2025-11-13",
          "model_name": "GPT-5.1",
          "score": 0.881,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-instant-2025-11-12",
          "model_name": "GPT-5.1 Instant",
          "score": 0.881,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-thinking-2025-11-12",
          "model_name": "GPT-5.1 Thinking",
          "score": 0.881,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "grok-4",
          "model_name": "Grok-4",
          "score": 0.875,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "claude-opus-4-5-20251101",
          "model_name": "Claude Opus 4.5",
          "score": 0.87,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "gemini-2.5-pro-preview-06-05",
          "model_name": "Gemini 2.5 Pro Preview 06-05",
          "score": 0.864,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.857,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "grok-4-fast",
          "model_name": "Grok 4 Fast",
          "score": 0.857,
          "rank": 9,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "humanity's-last-exam",
      "benchmark_name": "Humanity's Last Exam",
      "models": [
        {
          "model_id": "grok-4-heavy",
          "model_name": "Grok-4 Heavy",
          "score": 0.507,
          "rank": 1,
          "is_current_model": false
        },
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.458,
          "rank": 2,
          "is_current_model": true
        },
        {
          "model_id": "grok-4",
          "model_name": "Grok-4",
          "score": 0.4,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.248,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "gemini-2.5-pro-preview-06-05",
          "model_name": "Gemini 2.5 Pro Preview 06-05",
          "score": 0.216,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "deepseek-v3.2-exp",
          "model_name": "DeepSeek-V3.2-Exp",
          "score": 0.198,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-235b-a22b-thinking-2507",
          "model_name": "Qwen3-235B-A22B-Thinking-2507",
          "score": 0.182,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "gemini-2.5-pro",
          "model_name": "Gemini 2.5 Pro",
          "score": 0.178,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "deepseek-r1-0528",
          "model_name": "DeepSeek-R1-0528",
          "score": 0.177,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5-mini-2025-08-07",
          "model_name": "GPT-5 mini",
          "score": 0.167,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "mmmlu",
      "benchmark_name": "MMMLU",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.918,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "claude-opus-4-5-20251101",
          "model_name": "Claude Opus 4.5",
          "score": 0.908,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "claude-opus-4-1-20250805",
          "model_name": "Claude Opus 4.1",
          "score": 0.895,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "claude-sonnet-4-5-20250929",
          "model_name": "Claude Sonnet 4.5",
          "score": 0.891,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "claude-opus-4-20250514",
          "model_name": "Claude Opus 4",
          "score": 0.888,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "o1-2024-12-17",
          "model_name": "o1",
          "score": 0.877,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "gpt-4.1-2025-04-14",
          "model_name": "GPT-4.1",
          "score": 0.873,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-235b-a22b",
          "model_name": "Qwen3 235B A22B",
          "score": 0.867,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "claude-sonnet-4-20250514",
          "model_name": "Claude Sonnet 4",
          "score": 0.865,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "claude-3-7-sonnet-20250219",
          "model_name": "Claude 3.7 Sonnet",
          "score": 0.861,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "mmmu-pro",
      "benchmark_name": "MMMU-Pro",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.81,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.784,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "o3-2025-04-16",
          "model_name": "o3",
          "score": 0.764,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-thinking",
          "model_name": "Qwen3 VL 235B A22B Thinking",
          "score": 0.693,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-instruct",
          "model_name": "Qwen3 VL 235B A22B Instruct",
          "score": 0.681,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-thinking",
          "model_name": "Qwen3 VL 32B Thinking",
          "score": 0.681,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-instruct",
          "model_name": "Qwen3 VL 32B Instruct",
          "score": 0.653,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-30b-a3b-thinking",
          "model_name": "Qwen3 VL 30B A3B Thinking",
          "score": 0.63,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-30b-a3b-instruct",
          "model_name": "Qwen3 VL 30B A3B Instruct",
          "score": 0.604,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-8b-thinking",
          "model_name": "Qwen3 VL 8B Thinking",
          "score": 0.604,
          "rank": 9,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "screenspot-pro",
      "benchmark_name": "ScreenSpot Pro",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.727,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "qwen3-vl-235b-a22b-instruct",
          "model_name": "Qwen3 VL 235B A22B Instruct",
          "score": 0.62,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-thinking",
          "model_name": "Qwen3 VL 235B A22B Thinking",
          "score": 0.618,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-30b-a3b-instruct",
          "model_name": "Qwen3 VL 30B A3B Instruct",
          "score": 0.605,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-4b-instruct",
          "model_name": "Qwen3 VL 4B Instruct",
          "score": 0.595,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-instruct",
          "model_name": "Qwen3 VL 32B Instruct",
          "score": 0.579,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-30b-a3b-thinking",
          "model_name": "Qwen3 VL 30B A3B Thinking",
          "score": 0.573,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-thinking",
          "model_name": "Qwen3 VL 32B Thinking",
          "score": 0.571,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-8b-instruct",
          "model_name": "Qwen3 VL 8B Instruct",
          "score": 0.546,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-4b-thinking",
          "model_name": "Qwen3 VL 4B Thinking",
          "score": 0.492,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "simpleqa",
      "benchmark_name": "SimpleQA",
      "models": [
        {
          "model_id": "deepseek-v3.2-exp",
          "model_name": "DeepSeek-V3.2-Exp",
          "score": 0.971,
          "rank": 1,
          "is_current_model": false
        },
        {
          "model_id": "grok-4-fast",
          "model_name": "Grok 4 Fast",
          "score": 0.95,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "deepseek-v3.1",
          "model_name": "DeepSeek-V3.1",
          "score": 0.934,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "deepseek-r1-0528",
          "model_name": "DeepSeek-R1-0528",
          "score": 0.923,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.721,
          "rank": 5,
          "is_current_model": true
        },
        {
          "model_id": "gpt-4.5",
          "model_name": "GPT-4.5",
          "score": 0.625,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-thinking",
          "model_name": "Qwen3 VL 32B Thinking",
          "score": 0.554,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-235b-a22b-instruct-2507",
          "model_name": "Qwen3-235B-A22B-Instruct-2507",
          "score": 0.543,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "gemini-2.5-pro-preview-06-05",
          "model_name": "Gemini 2.5 Pro Preview 06-05",
          "score": 0.54,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-instruct",
          "model_name": "Qwen3 VL 235B A22B Instruct",
          "score": 0.519,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "swe-bench-verified",
      "benchmark_name": "SWE-Bench Verified",
      "models": [
        {
          "model_id": "claude-opus-4-5-20251101",
          "model_name": "Claude Opus 4.5",
          "score": 0.809,
          "rank": 1,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-2025-11-13",
          "model_name": "GPT-5.1",
          "score": 0.763,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-instant-2025-11-12",
          "model_name": "GPT-5.1 Instant",
          "score": 0.763,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5.1-thinking-2025-11-12",
          "model_name": "GPT-5.1 Thinking",
          "score": 0.763,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.762,
          "rank": 5,
          "is_current_model": true
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.749,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "claude-opus-4-1-20250805",
          "model_name": "Claude Opus 4.1",
          "score": 0.745,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "gpt-5-codex-2025-09-15",
          "model_name": "GPT-5 Codex",
          "score": 0.745,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "claude-haiku-4-5-20251015",
          "model_name": "Claude Haiku 4.5",
          "score": 0.733,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "claude-sonnet-4-20250514",
          "model_name": "Claude Sonnet 4",
          "score": 0.727,
          "rank": 10,
          "is_current_model": false
        }
      ]
    },
    {
      "benchmark_id": "videommmu",
      "benchmark_name": "VideoMMMU",
      "models": [
        {
          "model_id": "gemini-3-pro-preview",
          "model_name": "Gemini 3 Pro",
          "score": 0.876,
          "rank": 1,
          "is_current_model": true
        },
        {
          "model_id": "gpt-5-2025-08-07",
          "model_name": "GPT-5",
          "score": 0.846,
          "rank": 2,
          "is_current_model": false
        },
        {
          "model_id": "gemini-2.5-pro-preview-06-05",
          "model_name": "Gemini 2.5 Pro Preview 06-05",
          "score": 0.836,
          "rank": 3,
          "is_current_model": false
        },
        {
          "model_id": "o3-2025-04-16",
          "model_name": "o3",
          "score": 0.833,
          "rank": 4,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-thinking",
          "model_name": "Qwen3 VL 235B A22B Thinking",
          "score": 0.8,
          "rank": 5,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-32b-thinking",
          "model_name": "Qwen3 VL 32B Thinking",
          "score": 0.79,
          "rank": 6,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-30b-a3b-thinking",
          "model_name": "Qwen3 VL 30B A3B Thinking",
          "score": 0.75,
          "rank": 7,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-235b-a22b-instruct",
          "model_name": "Qwen3 VL 235B A22B Instruct",
          "score": 0.747,
          "rank": 8,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-8b-thinking",
          "model_name": "Qwen3 VL 8B Thinking",
          "score": 0.728,
          "rank": 9,
          "is_current_model": false
        },
        {
          "model_id": "qwen3-vl-4b-thinking",
          "model_name": "Qwen3 VL 4B Thinking",
          "score": 0.694,
          "rank": 10,
          "is_current_model": false
        }
      ]
    }
  ],
  "comparison_model": {
    "model_id": "grok-4-heavy",
    "name": "Grok-4 Heavy",
    "organization_name": "xAI",
    "release_date": null,
    "announcement_date": "2025-07-09",
    "knowledge_cutoff": "2024-12-31",
    "param_count": null,
    "multimodal": true,
    "license": {
      "name": "Proprietary",
      "allow_commercial": false
    },
    "benchmarks": {
      "aime-2025": 1.0,
      "gpqa": 0.884,
      "hmmt25": 0.967,
      "humanity's-last-exam": 0.507,
      "livecodebench": 0.794,
      "usamo25": 0.619
    },
    "provider": null
  }
}