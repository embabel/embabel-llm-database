[{"name":"Jamba 1.5 Large","id":"jamba-1.5-large","tags":["text-to-text"],"knowledgeCutoff":"2024-03-05","releaseDate":"2024-08-22","parameterCount":398000000000,"organization":{"id":"ai21","name":"AI21 Labs","website":"https://ai21.com"},"multiModal":false,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":2.0,"outputPerMillion":8.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":2.0,"outputPerMillion":8.0,"tags":[],"deprecated":false}],"description":"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality."},{"name":"Jamba 1.5 Mini","id":"jamba-1.5-mini","tags":["text-to-text"],"knowledgeCutoff":"2024-03-05","releaseDate":"2024-08-22","parameterCount":52000000000,"organization":{"id":"ai21","name":"AI21 Labs","website":"https://ai21.com"},"multiModal":false,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.2,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.2,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality."},{"name":"Qwen3 VL 235B A22B Instruct","id":"qwen3-vl-235b-a22b-instruct","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":236000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.3,"outputPerMillion":1.49,"tags":["fp8"],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.3,"outputPerMillion":1.5,"tags":["bf16"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 235B A22B Thinking","id":"qwen3-vl-235b-a22b-thinking","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":236000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"deepinfra-qwen3-vl-235b-a22b-thinking","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.45,"outputPerMillion":3.49,"tags":["fp8","text","image"],"deprecated":false},{"id":"novita-qwen3-vl-235b-a22b-thinking","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.98,"outputPerMillion":3.95,"tags":["bf16","text","image","video"],"deprecated":false}],"description":"Qwen3-VL-235B-A22B-Thinking is the most powerful vision-language model in the Qwen series, featuring 236B parameters with MoE architecture for reasoning-enhanced multimodal understanding. Key capabilities include: Visual Agent (operates PC/mobile GUIs, recognizes elements, invokes tools), Visual Coding (generates Draw.io/HTML/CSS/JS from images/videos), Advanced Spatial Perception (2D grounding and 3D grounding for spatial reasoning and embodied AI), Long Context & Video Understanding (native 256K context expandable to 1M, handles hours-long video with second-level indexing), Enhanced Multimodal Reasoning (excels in STEM/Math with causal analysis), Upgraded Visual Recognition (celebrities, anime, products, landmarks, flora/fauna), and Expanded OCR (32 languages, robust in low light/blur/tilt). Architecture innovations include Interleaved-MRoPE for positional embeddings, DeepStack for multi-level ViT feature fusion, and Text-Timestamp Alignment for precise video temporal modeling."},{"name":"Qwen3 VL 30B A3B Instruct","id":"qwen3-vl-30b-a3b-instruct","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":31000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"qwen3-vl-30b-a3b-instruct-novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.7,"tags":["bf16"],"deprecated":false},{"id":"qwen3-vl-30b-a3b-instruct-deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.29,"outputPerMillion":0.99,"tags":["fp8"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 30B A3B Thinking","id":"qwen3-vl-30b-a3b-thinking","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":31000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.2,"outputPerMillion":1.0,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.29,"outputPerMillion":0.99,"tags":[],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 32B Instruct","id":"qwen3-vl-32b-instruct","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":33000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 32B Thinking","id":"qwen3-vl-32b-thinking","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":33000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 4B Instruct","id":"qwen3-vl-4b-instruct","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":4000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"deepinfra/qwen3-vl-4b-instruct","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":0.6,"tags":["fp8","multimodal"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 4B Thinking","id":"qwen3-vl-4b-thinking","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":4000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"deepinfra/qwen3-vl-4b-thinking","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":1.0,"tags":["fp8","text","image"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 8B Instruct","id":"qwen3-vl-8b-instruct","tags":["image-text-to-text","video-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":9000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"qwen3-vl-8b-instruct","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.08,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"qwen3-vl-8b-instruct","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.18,"outputPerMillion":0.69,"tags":[],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3 VL 8B Thinking","id":"qwen3-vl-8b-thinking","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-22","parameterCount":9000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"deepinfra-qwen3-vl-8b-thinking","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.18,"outputPerMillion":2.09,"tags":["thinking"],"deprecated":false}],"description":"Qwen3-VL is a large multimodal model that unifies vision, language, and reasoning to achieve human-level perception and cognition across text, images, and video. Built on a 235B-parameter architecture, it integrates early joint training of visual and textual modalities for strong language grounding. The model supports up to a 1 million-token context window and excels at visual understanding, spatial reasoning, long video comprehension, and tool-based interaction. It can generate code from images, perform precise 2D/3D object grounding, and operate digital interfaces like a visual agent. The \"Instruct\" version rivals Gemini 2.5 Pro in perception benchmarks, while the \"Thinking\" version leads in multimodal reasoning and STEM tasks. With multilingual OCR, creative writing, and fine-grained scene interpretation, Qwen3-VL establishes a new open-source frontier for integrated vision-language intelligence."},{"name":"Qwen3-Next-80B-A3B-Base","id":"qwen3-next-80b-a3b-base","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-10","parameterCount":80000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen3-Next-80B-A3B-Base is the foundation model in the Qwen3-Next series, featuring revolutionary architectural innovations for ultimate training and inference efficiency. It introduces Hybrid Attention combining Gated DeltaNet (75% layers) and Gated Attention (25% layers) for efficient ultra-long context modeling, Ultra-Sparse MoE with 512 total experts but only 10 routed + 1 shared expert activated (3.7% activation ratio), and native Multi-Token Prediction for faster inference. With 80B total parameters and only ~3B activated per inference step, it achieves performance comparable to Qwen3-32B while using less than 10% training cost and delivering 10x+ throughput for 32K+ contexts. Trained on 15T tokens with training-stability-friendly designs including Zero-Centered RMSNorm and normalized MoE router parameters. Supports 256K context length, extensible to 1M tokens with YaRN scaling."},{"name":"Qwen3-Next-80B-A3B-Instruct","id":"qwen3-next-80b-a3b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-10","parameterCount":80000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"qwen3-next-80b-a3b-instruct","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.15,"outputPerMillion":1.5,"tags":["instruct"],"deprecated":false}],"description":"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE))."},{"name":"Qwen3-Next-80B-A3B-Thinking","id":"qwen3-next-80b-a3b-thinking","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-10","parameterCount":80000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.15,"outputPerMillion":1.5,"tags":["bf16"],"deprecated":false}],"description":"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks â€” outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content."},{"name":"Qwen3-235B-A22B-Thinking-2507","id":"qwen3-235b-a22b-thinking-2507","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-25","parameterCount":235000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.3,"outputPerMillion":3.0,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.3,"outputPerMillion":3.0,"tags":[],"deprecated":false}],"description":"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion."},{"name":"Qwen3-235B-A22B-Instruct-2507","id":"qwen3-235b-a22b-instruct-2507","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-22","parameterCount":235000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"qwen3-235b-a22b-instruct-2507","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.15,"outputPerMillion":0.8,"tags":[],"deprecated":false},{"id":"qwen3-235b-a22b-instruct-2507","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.15,"outputPerMillion":0.8,"tags":[],"deprecated":false}],"description":"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks."},{"name":"Qwen3 235B A22B","id":"qwen3-235b-a22b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-04-29","parameterCount":235000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"fireworks-qwen3-235b-a22b","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false},{"id":"deepinfra-qwen3-235b-a22b","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.2,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"novita-qwen3-235b-a22b","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.8,"tags":[],"deprecated":false},{"id":"together-qwen3-235b-a22b","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.6,"tags":[],"deprecated":false}],"description":"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models."},{"name":"Qwen3 30B A3B","id":"qwen3-30b-a3b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-04-29","parameterCount":30500000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":0.3,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.44,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false}],"description":"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters."},{"name":"Qwen3 32B","id":"qwen3-32b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-04-29","parameterCount":32800000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":0.3,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.44,"tags":[],"deprecated":false},{"id":"sambanova","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":0.4,"outputPerMillion":0.8,"tags":[],"deprecated":false}],"description":"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities."},{"name":"Qwen2.5-Omni-7B","id":"qwen2.5-omni-7b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-27","parameterCount":7000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture."},{"name":"QwQ-32B","id":"qwq-32b","tags":["text-to-text"],"knowledgeCutoff":"2024-11-28","releaseDate":"2025-03-05","parameterCount":32500000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns."},{"name":"Qwen2.5 VL 32B Instruct","id":"qwen2.5-vl-32b","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-02-28","parameterCount":33500000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation."},{"name":"Qwen3-Coder 480B A35B Instruct","id":"qwen3-coder-480b-a35b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-31","parameterCount":480000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.4,"outputPerMillion":1.6,"tags":[],"deprecated":false}],"description":"Qwen3-Coder-480B-A35B-Instruct is Qwen's most agentic code model to date, featuring 480 billion total parameters with 35 billion activated parameters using MoE architecture. It achieves significant performance among open models on Agentic Coding, Agentic Browser-Use, and foundational coding tasks, with results comparable to Claude Sonnet. Features native 256K token context length (extendable to 1M tokens with Yarn), optimized for repository-scale understanding, and specialized function call format for agentic coding platforms like Qwen Code and CLINE."},{"name":"Qwen2.5 VL 72B Instruct","id":"qwen2.5-vl-72b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-26","parameterCount":72000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents."},{"name":"Qwen2.5 VL 7B Instruct","id":"qwen2.5-vl-7b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-26","parameterCount":8290000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation."},{"name":"QvQ-72B-Preview","id":"qvq-72b-preview","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-25","parameterCount":73400000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops."},{"name":"QwQ-32B-Preview","id":"qwq-32b-preview","tags":["text-to-text"],"knowledgeCutoff":"2024-11-28","releaseDate":"2024-11-28","parameterCount":32500000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"deepinfra-qwq-32b-preview","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"hyperbolic-qwq-32b-preview","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"fireworks-qwq-32b-preview","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"together-qwq-32b-preview","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":1.2,"outputPerMillion":1.2,"tags":[],"deprecated":false}],"description":"An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns."},{"name":"Qwen2.5 14B Instruct","id":"qwen-2.5-14b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":14700000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more."},{"name":"Qwen2.5 32B Instruct","id":"qwen-2.5-32b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":32500000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages."},{"name":"Qwen2.5 72B Instruct","id":"qwen-2.5-72b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":72700000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.35,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.4,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":1.2,"outputPerMillion":1.2,"tags":[],"deprecated":false}],"description":"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages."},{"name":"Qwen2.5 7B Instruct","id":"qwen-2.5-7b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":7610000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"together-qwen-2.5-7b-instruct","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.3,"outputPerMillion":0.3,"tags":[],"deprecated":false}],"description":"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more."},{"name":"Qwen2.5-Coder 32B Instruct","id":"qwen-2.5-coder-32b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":32000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.09,"outputPerMillion":0.09,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.18,"outputPerMillion":0.18,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false}],"description":"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities."},{"name":"Qwen2.5-Coder 7B Instruct","id":"qwen-2.5-coder-7b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-19","parameterCount":7000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning."},{"name":"Qwen2-VL-72B-Instruct","id":"qwen2-vl-72b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2023-06-30","releaseDate":"2024-08-29","parameterCount":73400000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts."},{"name":"Qwen2 72B Instruct","id":"qwen2-72b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-23","parameterCount":72000000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks."},{"name":"Qwen2 7B Instruct","id":"qwen2-7b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-23","parameterCount":7620000000,"organization":{"id":"qwen","name":"Alibaba Cloud / Qwen Team","website":"https://qwenlm.github.io"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens."},{"name":"Nova Lite","id":"nova-lite","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-11-20","parameterCount":0,"organization":{"id":"amazon","name":"Amazon","website":"https://aws.amazon.com"},"multiModal":true,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.06,"outputPerMillion":0.24,"tags":[],"deprecated":false}],"description":"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text."},{"name":"Nova Micro","id":"nova-micro","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-11-20","parameterCount":0,"organization":{"id":"amazon","name":"Amazon","website":"https://aws.amazon.com"},"multiModal":false,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.03,"outputPerMillion":0.14,"tags":[],"deprecated":false}],"description":"A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks."},{"name":"Nova Pro","id":"nova-pro","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-11-20","parameterCount":0,"organization":{"id":"amazon","name":"Amazon","website":"https://aws.amazon.com"},"multiModal":true,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.8,"outputPerMillion":3.2,"tags":[],"deprecated":false}],"description":"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency."},{"name":"Claude Opus 4.5","id":"claude-opus-4-5-20251101","tags":["image-text-to-text"],"knowledgeCutoff":"2025-03-31","releaseDate":"2025-11-24","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":5.0,"outputPerMillion":25.0,"tags":[],"deprecated":false}],"description":"Premium model combining maximum intelligence with practical performance. Best model in the world for coding, agents, and computer use. Most robustly aligned model with best prompt injection resistance of any frontier model. Features extended thinking, 200K context window, 64K max output, and a new effort parameter for controlling reasoning depth. Pricing: $5/$25 per million tokens (input/output)."},{"name":"Claude Haiku 4.5","id":"claude-haiku-4-5-20251015","tags":["image-text-to-text"],"knowledgeCutoff":"2025-02-01","releaseDate":"2025-10-15","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":1.0,"outputPerMillion":5.0,"tags":[],"deprecated":false}],"description":"Claude Haiku 4.5 is Anthropic's fastest, most cost-efficient model, matching Sonnet 4's performance on coding, computer use, and agent tasks. It offers similar performance to Sonnet 4 at one-third the cost and more than twice the speed, making it ideal for high-volume, latency-sensitive applications and multi-agent orchestration."},{"name":"Claude Sonnet 4.5","id":"claude-sonnet-4-5-20250929","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2025-01-31","releaseDate":"2025-09-29","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It's the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities."},{"name":"Claude Opus 4.1","id":"claude-opus-4-1-20250805","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-05","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false}],"description":"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects."},{"name":"Claude Opus 4","id":"claude-opus-4-20250514","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-22","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false}],"description":"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities."},{"name":"Claude Sonnet 4","id":"claude-sonnet-4-20250514","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-22","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use."},{"name":"Claude 3.7 Sonnet","id":"claude-3-7-sonnet-20250219","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-02-24","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development."},{"name":"Claude 3.5 Haiku","id":"claude-3-5-haiku-20241022","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-10-22","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":false,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.8,"outputPerMillion":4.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.8,"outputPerMillion":4.0,"tags":[],"deprecated":false},{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":1.0,"outputPerMillion":5.0,"tags":[],"deprecated":false}],"description":"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation."},{"name":"Claude 3.5 Sonnet","id":"claude-3-5-sonnet-20241022","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-10-22","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user."},{"name":"Claude 3.5 Sonnet","id":"claude-3-5-sonnet-20240620","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-06-21","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions."},{"name":"Claude 3 Haiku","id":"claude-3-haiku-20240307","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-03-13","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":0.25,"outputPerMillion":1.25,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.25,"outputPerMillion":1.25,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.25,"outputPerMillion":1.25,"tags":[],"deprecated":false}],"description":"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions."},{"name":"Claude 3 Opus","id":"claude-3-opus-20240229","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-02-29","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":15.0,"outputPerMillion":75.0,"tags":[],"deprecated":false}],"description":"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI."},{"name":"Claude 3 Sonnet","id":"claude-3-sonnet-20240229","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-02-29","parameterCount":0,"organization":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"multiModal":true,"modelProviders":[{"id":"anthropic","provider":{"id":"anthropic","name":"Anthropic","website":"https://anthropic.com"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Claude 3 Sonnet strikes the ideal balance between intelligence and speedâ€”particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments."},{"name":"Command R+","id":"command-r-plus-04-2024","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-30","parameterCount":104000000000,"organization":{"id":"cohere","name":"Cohere","website":"https://cohere.ai"},"multiModal":false,"modelProviders":[{"id":"cohere","provider":{"id":"cohere","name":"Cohere","website":"https://cohere.ai"},"inputPerMillion":0.25,"outputPerMillion":1.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks."},{"name":"DeepSeek-V3.2-Exp","id":"deepseek-v3.2-exp","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-29","parameterCount":685000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.27,"outputPerMillion":0.41,"tags":[],"deprecated":false}],"description":"DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing."},{"name":"DeepSeek-R1-0528","id":"deepseek-r1-0528","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-28","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.5,"outputPerMillion":2.15,"tags":[],"deprecated":false},{"id":"deepseek","provider":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com/"},"inputPerMillion":0.55,"outputPerMillion":2.19,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.7,"outputPerMillion":2.5,"tags":[],"deprecated":false}],"description":"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach."},{"name":"DeepSeek-V3 0324","id":"deepseek-v3-0324","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-25","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepseek-v3-0324","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.28,"outputPerMillion":1.14,"tags":[],"deprecated":false}],"description":"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks."},{"name":"DeepSeek-R1","id":"deepseek-r1","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepseek","provider":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com/"},"inputPerMillion":0.55,"outputPerMillion":2.19,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.85,"outputPerMillion":2.5,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":7.0,"outputPerMillion":7.0,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":8.0,"outputPerMillion":8.0,"tags":[],"deprecated":false}],"description":"DeepSeek-R1 is a reasoning-focused language model from DeepSeek that features advanced thinking capabilities. It serves as the foundation for DeepSeek's reasoning model family and pioneered their thinking mode approach for complex problem-solving tasks."},{"name":"DeepSeek R1 Distill Llama 70B","id":"deepseek-r1-distill-llama-70b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":70600000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Distill Llama 8B","id":"deepseek-r1-distill-llama-8b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":8030000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Distill Qwen 14B","id":"deepseek-r1-distill-qwen-14b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":14800000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Distill Qwen 1.5B","id":"deepseek-r1-distill-qwen-1.5b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":1780000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Distill Qwen 32B","id":"deepseek-r1-distill-qwen-32b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":32800000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.12,"outputPerMillion":0.18,"tags":[],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Distill Qwen 7B","id":"deepseek-r1-distill-qwen-7b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":7620000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks."},{"name":"DeepSeek R1 Zero","id":"deepseek-r1-zero","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks."},{"name":"DeepSeek-V3.1","id":"deepseek-v3.1","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-10","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.27,"outputPerMillion":1.0,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.27,"outputPerMillion":1.0,"tags":[],"deprecated":false}],"description":"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving."},{"name":"DeepSeek-V3","id":"deepseek-v3","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-25","parameterCount":671000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepseek-v3","provider":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com/"},"inputPerMillion":0.27,"outputPerMillion":1.1,"tags":[],"deprecated":false}],"description":"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks."},{"name":"DeepSeek VL2","id":"deepseek-vl2","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-13","parameterCount":27000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":true,"modelProviders":[{"id":"replicate","provider":{"id":"replicate","name":"Replicate","website":"https://replicate.com/"},"inputPerMillion":9.5,"outputPerMillion":4800.0,"tags":[],"deprecated":false}],"description":"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding."},{"name":"DeepSeek VL2 Small","id":"deepseek-vl2-small","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-13","parameterCount":16000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding."},{"name":"DeepSeek VL2 Tiny","id":"deepseek-vl2-tiny","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-13","parameterCount":3000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding."},{"name":"DeepSeek-V2.5","id":"deepseek-v2.5","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-05-08","parameterCount":236000000000,"organization":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com"},"multiModal":false,"modelProviders":[{"id":"deepseek","provider":{"id":"deepseek","name":"DeepSeek","website":"https://deepseek.com/"},"inputPerMillion":0.14,"outputPerMillion":0.28,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.7,"outputPerMillion":1.4,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":2.0,"outputPerMillion":2.0,"tags":[],"deprecated":false}],"description":"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following."},{"name":"Gemini 3 Pro","id":"gemini-3-pro-preview","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2025-01-31","releaseDate":"2025-11-18","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":2.0,"outputPerMillion":12.0,"tags":[],"deprecated":false}],"description":"Gemini 3 Pro is the first model in the new Gemini 3 series. It is best for complex tasks that require broad world knowledge and advanced reasoning across modalities. Gemini 3 Pro uses dynamic thinking by default to reason through prompts, and features a 1 million-token input context window with 64k output tokens."},{"name":"Gemma 3n E2B","id":"gemma-3n-e2b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-06-26","parameterCount":8000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages."},{"name":"Gemma 3n E2B Instructed","id":"gemma-3n-e2b-it","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-06-26","parameterCount":8000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages."},{"name":"Gemma 3n E4B","id":"gemma-3n-e4b","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-06-26","parameterCount":8000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages."},{"name":"Gemma 3n E4B Instructed","id":"gemma-3n-e4b-it","tags":["image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-06-26","parameterCount":8000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"together-gemma-3n-e4b-it","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":20.0,"outputPerMillion":40.0,"tags":[],"deprecated":false}],"description":"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages."},{"name":"Gemini 2.5 Flash-Lite","id":"gemini-2.5-flash-lite","tags":["image-text-to-text"],"knowledgeCutoff":"2025-01-01","releaseDate":"2025-06-17","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.1,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length."},{"name":"Gemini 2.5 Pro Preview 06-05","id":"gemini-2.5-pro-preview-06-05","tags":["image-text-to-text"],"knowledgeCutoff":"2025-01-31","releaseDate":"2025-06-05","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio."},{"name":"Gemini 2.5 Flash","id":"gemini-2.5-flash","tags":["image-text-to-text"],"knowledgeCutoff":"2025-01-31","releaseDate":"2025-05-20","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.3,"outputPerMillion":2.5,"tags":[],"deprecated":false}],"description":"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window."},{"name":"Gemini 2.5 Pro","id":"gemini-2.5-pro","tags":["image-text-to-text"],"knowledgeCutoff":"2025-01-31","releaseDate":"2025-05-20","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"A highly capable AI model from Google, designed for the agentic era. Gemini 2.5 Pro performs well on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window."},{"name":"Gemini Diffusion","id":"gemini-diffusion","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-20","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts."},{"name":"Gemma 3n E2B Instructed LiteRT (Preview)","id":"gemma-3n-e2b-it-litert-preview","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-05-20","parameterCount":1910000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use."},{"name":"Gemma 3n E4B Instructed LiteRT Preview","id":"gemma-3n-e4b-it-litert-preview","tags":["image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-05-20","parameterCount":1910000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use."},{"name":"MedGemma 4B IT","id":"medgemma-4b-it","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-20","parameterCount":4300000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma."},{"name":"Gemma 3 12B","id":"gemma-3-12b-it","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-12","parameterCount":12000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra/gemma-3-12b-it","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.05,"outputPerMillion":0.1,"tags":[],"deprecated":false}],"description":"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks."},{"name":"Gemma 3 1B","id":"gemma-3-1b-it","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-12","parameterCount":1000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices."},{"name":"Gemma 3 27B","id":"gemma-3-27b-it","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-12","parameterCount":27000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"gemma-3-27b-it","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.1,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"gemma-3-27b-it","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.11,"outputPerMillion":0.2,"tags":[],"deprecated":false}],"description":"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks."},{"name":"Gemma 3 4B","id":"gemma-3-4b-it","tags":["image-text-to-text"],"knowledgeCutoff":"2024-08-01","releaseDate":"2025-03-12","parameterCount":4000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra/gemma-3-4b-it","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.02,"outputPerMillion":0.04,"tags":["multimodal","instruct"],"deprecated":false}],"description":"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks."},{"name":"Gemini 2.0 Flash-Lite","id":"gemini-2.0-flash-lite","tags":["image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-02-05","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.07,"outputPerMillion":0.3,"tags":[],"deprecated":false}],"description":"A Gemini 2.0 Flash model optimized for cost efficiency and low latency"},{"name":"Gemini 2.0 Flash Thinking","id":"gemini-2.0-flash-thinking","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-08-01","releaseDate":"2025-01-21","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemini 2.0 Flash Thinking is a enhanced reasoning model, capable of showing its thoughts to improve performance and explainability. Combining speed and performance, Gemini 2.0 Flash Thinking also excels in science and math, showing its thinking to solve complex problems."},{"name":"Gemini 2.0 Flash","id":"gemini-2.0-flash","tags":["image-text-to-text"],"knowledgeCutoff":"2024-08-01","releaseDate":"2024-12-01","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.1,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations."},{"name":"Gemma 2 27B","id":"gemma-2-27b-it","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-06-27","parameterCount":27200000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning."},{"name":"Gemma 2 9B","id":"gemma-2-9b-it","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-06-27","parameterCount":9240000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP."},{"name":"Gemini 1.5 Flash","id":"gemini-1.5-flash","tags":["image-text-to-text"],"knowledgeCutoff":"2023-11-01","releaseDate":"2024-05-01","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false}],"description":"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks."},{"name":"Gemini 1.5 Pro","id":"gemini-1.5-pro","tags":["image-text-to-text"],"knowledgeCutoff":"2023-11-01","releaseDate":"2024-05-01","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":2.5,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text."},{"name":"Gemini 1.5 Flash 8B","id":"gemini-1.5-flash-8b","tags":["image-text-to-text"],"knowledgeCutoff":"2024-10-01","releaseDate":"2024-03-15","parameterCount":8000000000,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":true,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.07,"outputPerMillion":0.3,"tags":[],"deprecated":false}],"description":"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters."},{"name":"Gemini 1.0 Pro","id":"gemini-1.0-pro","tags":["text-to-text"],"knowledgeCutoff":"2024-02-01","releaseDate":"2024-02-15","parameterCount":0,"organization":{"id":"google","name":"Google","website":"https://google.com"},"multiModal":false,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.5,"outputPerMillion":1.5,"tags":[],"deprecated":false}],"description":"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024."},{"name":"IBM Granite 4.0 Tiny Preview","id":"granite-4.0-tiny-preview","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-05-02","parameterCount":7000000000,"organization":{"id":"ibm","name":"IBM","website":"https://ibm.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding."},{"name":"Granite 3.3 8B Base","id":"granite-3.3-8b-base","tags":["text-to-text"],"knowledgeCutoff":"2024-04-01","releaseDate":"2025-04-16","parameterCount":8170000000,"organization":{"id":"ibm","name":"IBM","website":"https://ibm.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks"},{"name":"Granite 3.3 8B Instruct","id":"granite-3.3-8b-instruct","tags":["text-to-text"],"knowledgeCutoff":"2024-04-01","releaseDate":"2025-04-16","parameterCount":8000000000,"organization":{"id":"ibm","name":"IBM","website":"https://ibm.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license."},{"name":"Llama 4 Maverick","id":"llama-4-maverick","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-04-05","parameterCount":400000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.17,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.17,"outputPerMillion":0.85,"tags":[],"deprecated":false},{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.18,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.2,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.22,"outputPerMillion":0.88,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.27,"outputPerMillion":0.85,"tags":[],"deprecated":false},{"id":"sambanova","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":0.63,"outputPerMillion":1.79,"tags":[],"deprecated":false}],"description":"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window."},{"name":"Llama 4 Scout","id":"llama-4-scout","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-04-05","parameterCount":109000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.08,"outputPerMillion":0.3,"tags":[],"deprecated":false},{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.08,"outputPerMillion":0.3,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.11,"outputPerMillion":0.34,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.18,"outputPerMillion":0.59,"tags":[],"deprecated":false}],"description":"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window."},{"name":"Llama 3.3 70B Instruct","id":"llama-3.3-70b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-06","parameterCount":70000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":false,"modelProviders":[{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.23,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.4,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.59,"outputPerMillion":7.9,"tags":[],"deprecated":false},{"id":"sambanova","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":0.6,"outputPerMillion":1.2,"tags":[],"deprecated":false},{"id":"cerebras","provider":{"id":"cerebras","name":"Cerebras","website":"https://cerebras.ai"},"inputPerMillion":0.7,"outputPerMillion":0.8,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.72,"outputPerMillion":0.72,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.88,"outputPerMillion":0.88,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false}],"description":"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages."},{"name":"Llama 3.2 11B Instruct","id":"llama-3.2-11b-instruct","tags":["image-text-to-text"],"knowledgeCutoff":"2023-12-31","releaseDate":"2024-09-25","parameterCount":10600000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra-llama-3.2-11b-instruct","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.05,"outputPerMillion":0.05,"tags":[],"deprecated":false},{"id":"sambanova-llama-3.2-11b-instruct","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":0.15,"outputPerMillion":0.3,"tags":[],"deprecated":false},{"id":"bedrock-llama-3.2-11b-instruct","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.16,"outputPerMillion":0.16,"tags":[],"deprecated":false},{"id":"groq-llama-3.2-11b-instruct","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.18,"outputPerMillion":0.18,"tags":[],"deprecated":false},{"id":"together-llama-3.2-11b-instruct","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.18,"outputPerMillion":0.18,"tags":[],"deprecated":false},{"id":"fireworks-llama-3.2-11b-instruct","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false}],"description":"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output."},{"name":"Llama 3.2 3B Instruct","id":"llama-3.2-3b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-25","parameterCount":3210000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.01,"outputPerMillion":0.02,"tags":[],"deprecated":false}],"description":"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge."},{"name":"Llama 3.2 90B Instruct","id":"llama-3.2-90b-instruct","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-25","parameterCount":90000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.35,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.72,"outputPerMillion":0.72,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":1.2,"outputPerMillion":1.2,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":2.0,"outputPerMillion":2.0,"tags":[],"deprecated":false}],"description":"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks."},{"name":"Llama 3.1 405B Instruct","id":"llama-3.1-405b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-23","parameterCount":405000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":false,"modelProviders":[{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":1.79,"outputPerMillion":1.79,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":3.0,"outputPerMillion":3.0,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":3.0,"outputPerMillion":3.0,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":3.5,"outputPerMillion":3.5,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":4.0,"outputPerMillion":4.0,"tags":[],"deprecated":false},{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":5.0,"outputPerMillion":16.0,"tags":[],"deprecated":false},{"id":"replicate","provider":{"id":"replicate","name":"Replicate","website":"https://replicate.com/"},"inputPerMillion":9.5,"outputPerMillion":9.5,"tags":[],"deprecated":false}],"description":"Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length."},{"name":"Llama 3.1 70B Instruct","id":"llama-3.1-70b-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-23","parameterCount":70000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":false,"modelProviders":[{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.35,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.4,"outputPerMillion":0.4,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.59,"outputPerMillion":0.78,"tags":[],"deprecated":false},{"id":"cerebras","provider":{"id":"cerebras","name":"Cerebras","website":"https://cerebras.ai"},"inputPerMillion":0.6,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.89,"outputPerMillion":0.89,"tags":[],"deprecated":false},{"id":"sambanova","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":5.0,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks."},{"name":"Llama 3.1 8B Instruct","id":"llama-3.1-8b-instruct","tags":["text-to-text"],"knowledgeCutoff":"2023-12-31","releaseDate":"2024-07-23","parameterCount":8000000000,"organization":{"id":"meta","name":"Meta","website":"https://meta.com"},"multiModal":false,"modelProviders":[{"id":"lambda","provider":{"id":"lambda","name":"Lambda","website":"https://lambdalabs.com"},"inputPerMillion":0.03,"outputPerMillion":0.03,"tags":[],"deprecated":false},{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.05,"outputPerMillion":0.05,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.05,"outputPerMillion":0.08,"tags":[],"deprecated":false},{"id":"sambanova","provider":{"id":"sambanova","name":"Sambanova","website":"https://sambanova.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"cerebras","provider":{"id":"cerebras","name":"Cerebras","website":"https://cerebras.ai"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false},{"id":"hyperbolic","provider":{"id":"hyperbolic","name":"Hyperbolic","website":"https://hyperbolic.xyz"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false},{"id":"together","provider":{"id":"together","name":"Together","website":"https://together.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.2,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"bedrock","provider":{"id":"bedrock","name":"Bedrock","website":"https://aws.amazon.com/bedrock/"},"inputPerMillion":0.22,"outputPerMillion":0.22,"tags":[],"deprecated":false}],"description":"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities."},{"name":"Phi 4 Mini Reasoning","id":"phi-4-mini-reasoning","tags":["text-to-text"],"knowledgeCutoff":"2025-02-01","releaseDate":"2025-04-30","parameterCount":3800000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking."},{"name":"Phi 4 Reasoning","id":"phi-4-reasoning","tags":["text-to-text"],"knowledgeCutoff":"2025-03-01","releaseDate":"2025-04-30","parameterCount":14000000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills."},{"name":"Phi 4 Reasoning Plus","id":"phi-4-reasoning-plus","tags":["text-to-text"],"knowledgeCutoff":"2025-03-01","releaseDate":"2025-04-30","parameterCount":14000000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency."},{"name":"Phi 4 Mini","id":"phi-4-mini","tags":["text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-02-01","parameterCount":3840000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization."},{"name":"Phi-4-multimodal-instruct","id":"phi-4-multimodal-instruct","tags":["image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-02-01","parameterCount":5600000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":true,"modelProviders":[{"id":"deepinfra-phi-4-multimodal-instruct","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.05,"outputPerMillion":0.1,"tags":[],"deprecated":false}],"description":"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety."},{"name":"Phi 4","id":"phi-4","tags":["text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2024-12-12","parameterCount":14700000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.07,"outputPerMillion":0.14,"tags":[],"deprecated":false}],"description":"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety."},{"name":"Phi-3.5-mini-instruct","id":"phi-3.5-mini-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-23","parameterCount":3800000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false}],"description":"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license."},{"name":"Phi-3.5-MoE-instruct","id":"phi-3.5-moe-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-23","parameterCount":60000000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks."},{"name":"Phi-3.5-vision-instruct","id":"phi-3.5-vision-instruct","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-23","parameterCount":4200000000,"organization":{"id":"microsoft","name":"Microsoft","website":"https://microsoft.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license."},{"name":"MiniMax M2","id":"minimax-m2","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-10-27","parameterCount":230000000000,"organization":{"id":"minimax","name":"MiniMax","website":"https://www.minimaxi.com/"},"multiModal":false,"modelProviders":[{"id":"minimax","provider":{"id":"minimax","name":"MiniMax","website":"https://platform.minimax.io"},"inputPerMillion":0.3,"outputPerMillion":1.2,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.3,"outputPerMillion":1.2,"tags":[],"deprecated":false}],"description":"MiniMax M2 is an open-source large language model by MiniMax, built for agents and coding tasks. It delivers state-of-the-art tool use, reasoning, and search performance while maintaining exceptional cost-efficiency and speed, priced at just 8% of Claude 3.5 Sonnet's cost and running at nearly double its inference speed (â‰ˆ100 TPS). Designed for end-to-end agentic workflows, it excels at long-chain tool calling across Shell, Browser, Python, and other MCP tools. While slightly behind top overseas models in programming, it ranks among the best domestic models and top five globally on the Artificial Analysis benchmark. M2 powers the MiniMax Agent platform, available in Lightning Mode for fast tasks and Pro Mode for complex multi-step reasoning, and its weights, API, and deployment guides are freely available on Hugging Face, vLLM, and SGLang."},{"name":"MiniMax M1 40K","id":"minimax-m1-40k","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-06-16","parameterCount":456000000000,"organization":{"id":"minimax","name":"MiniMax","website":"https://www.minimaxi.com/"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"MiniMax-M1 is an open-source, large-scale reasoning model that uses a hybrid-attention architecture for efficient long-context processing. It supports up to a 1 million token context window and 80,000-token reasoning output, matching Gemini 2.5 Pro's scale while being highly cost-effective. Its Lightning Attention mechanism reduces compute requirements to about 30% of DeepSeek R1's, and a new reinforcement learning algorithm, CISPO, doubles convergence speed compared to other RL methods. Trained on 512 H800s over three weeks, M1 achieves near state-of-the-art results across software engineering, long-context, and tool-use benchmarks, outperforming most open models and rivaling top closed systems."},{"name":"MiniMax M1 80K","id":"minimax-m1-80k","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-06-16","parameterCount":456000000000,"organization":{"id":"minimax","name":"MiniMax","website":"https://www.minimaxi.com/"},"multiModal":false,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.55,"outputPerMillion":2.2,"tags":[],"deprecated":false}],"description":"MiniMax-M1 is an open-source, large-scale reasoning model that uses a hybrid-attention architecture for efficient long-context processing. It supports up to a 1 million token context window and 80,000-token reasoning output, matching Gemini 2.5 Pro's scale while being highly cost-effective. Its Lightning Attention mechanism reduces compute requirements to about 30% of DeepSeek R1's, and a new reinforcement learning algorithm, CISPO, doubles convergence speed compared to other RL methods. Trained on 512 H800s over three weeks, M1 achieves near state-of-the-art results across software engineering, long-context, and tool-use benchmarks, outperforming most open models and rivaling top closed systems."},{"name":"Devstral Small 1.1","id":"devstral-small-2507","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-11","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.1,"outputPerMillion":0.3,"tags":[],"deprecated":false}],"description":"Devstral Small 1.1 (also called devstral-small-2507) is based on the Mistral-Small-3.1 foundation model and contains approximately 24 billion parameters. It supports a 128k token context window, which allows it to handle multi-file code inputs and long prompts typical in software engineering workflows. The model is fine-tuned specifically for structured outputs, including XML and function-calling formats. This makes it compatible with agent frameworks such as OpenHands and suitable for tasks like program navigation, multi-step edits, and code search. It is licensed under Apache 2.0 and available for both research and commercial use."},{"name":"Devstral Medium","id":"devstral-medium-2507","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-10","parameterCount":0,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.4,"outputPerMillion":2.0,"tags":[],"deprecated":false}],"description":"Devstral Medium builds upon the strengths of Devstral Small and takes performance to the next level with a score of 61.6% on SWE-Bench Verified. Devstral Medium is available through the Mistral public API, and offers exceptional performance at a competitive price point, making it an ideal choice for businesses and developers looking for a high-quality, cost-effective model."},{"name":"Mistral Small 3.2 24B Instruct","id":"mistral-small-3.2-24b-instruct-2506","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2023-10-01","releaseDate":"2025-06-20","parameterCount":23600000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503."},{"name":"Magistral Medium","id":"magistral-medium","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2025-06-01","releaseDate":"2025-06-10","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency."},{"name":"Magistral Small 2506","id":"magistral-small-2506","tags":["text-to-text"],"knowledgeCutoff":"2025-06-01","releaseDate":"2025-06-10","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized."},{"name":"Mistral Small 3.1 24B Base","id":"mistral-small-3.1-24b-base-2503","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-17","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"mistral-small-3.1-24b-base-2503","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.1,"outputPerMillion":0.3,"tags":["base"],"deprecated":false}],"description":"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning."},{"name":"Mistral Small 3.1 24B Instruct","id":"mistral-small-3.1-24b-instruct-2503","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-03-17","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks."},{"name":"Mistral Small 3 24B Base","id":"mistral-small-24b-base-2501","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2023-10-01","releaseDate":"2025-01-30","parameterCount":23600000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware."},{"name":"Mistral Small 3 24B Instruct","id":"mistral-small-24b-instruct-2501","tags":["text-to-text"],"knowledgeCutoff":"2023-10-01","releaseDate":"2025-01-30","parameterCount":24000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.07,"outputPerMillion":0.14,"tags":[],"deprecated":false},{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.1,"outputPerMillion":0.3,"tags":[],"deprecated":false}],"description":"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2â€“3Ã— its size while using significantly fewer compute resources."},{"name":"Pixtral Large","id":"pixtral-large","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-11-18","parameterCount":124000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":2.0,"outputPerMillion":6.0,"tags":[],"deprecated":false}],"description":"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images."},{"name":"Ministral 8B Instruct","id":"ministral-8b-instruct-2410","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-10-16","parameterCount":8019808256,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.1,"outputPerMillion":0.1,"tags":[],"deprecated":false}],"description":"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size."},{"name":"Mistral Small","id":"mistral-small-2409","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-17","parameterCount":22000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.2,"outputPerMillion":0.6,"tags":[],"deprecated":false}],"description":"An enterprise-grade 22B parameter model optimized for tasks like translation, summarization, and sentiment analysis. Offers significant improvements in human alignment, reasoning capabilities, and code generation compared to previous versions."},{"name":"Pixtral-12B","id":"pixtral-12b-2409","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-17","parameterCount":12400000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":true,"modelProviders":[{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.15,"outputPerMillion":0.15,"tags":[],"deprecated":false}],"description":"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context."},{"name":"Mistral Large 2","id":"mistral-large-2-2407","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-24","parameterCount":123000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":2.0,"outputPerMillion":6.0,"tags":[],"deprecated":false},{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":2.0,"outputPerMillion":6.0,"tags":[],"deprecated":false}],"description":"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs."},{"name":"Mistral NeMo Instruct","id":"mistral-nemo-instruct-2407","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-07-18","parameterCount":12000000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"google","provider":{"id":"google","name":"Google","website":"https://ai.google.dev"},"inputPerMillion":0.15,"outputPerMillion":0.15,"tags":[],"deprecated":false},{"id":"mistral","provider":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"inputPerMillion":0.15,"outputPerMillion":0.15,"tags":[],"deprecated":false}],"description":"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages."},{"name":"Codestral-22B","id":"codestral-22b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-05-29","parameterCount":22200000000,"organization":{"id":"mistral","name":"Mistral AI","website":"https://mistral.ai"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks."},{"name":"Kimi K2 0905","id":"kimi-k2-0905","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-05","parameterCount":1000000000000,"organization":{"id":"moonshotai","name":"Moonshot AI","website":"https://moonshot.cn"},"multiModal":false,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.6,"outputPerMillion":2.5,"tags":[],"deprecated":false}],"description":"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training."},{"name":"Kimi K2-Instruct-0905","id":"kimi-k2-instruct-0905","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-05","parameterCount":1000000000000,"organization":{"id":"moonshotai","name":"Moonshot AI","website":"https://moonshot.cn"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly."},{"name":"Kimi K2 Base","id":"kimi-k2-base","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-11","parameterCount":1000000000000,"organization":{"id":"moonshotai","name":"Moonshot AI","website":"https://moonshot.cn"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities."},{"name":"Kimi K2 Instruct","id":"kimi-k2-instruct","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-11","parameterCount":1000000000000,"organization":{"id":"moonshotai","name":"Moonshot AI","website":"https://moonshot.cn"},"multiModal":false,"modelProviders":[{"id":"kimi-k2-instruct","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.5,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"kimi-k2-instruct","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.57,"outputPerMillion":2.3,"tags":[],"deprecated":false}],"description":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking."},{"name":"Kimi-k1.5","id":"kimi-k1.5","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-01-20","parameterCount":0,"organization":{"id":"moonshotai","name":"Moonshot AI","website":"https://moonshot.cn"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks."},{"name":"Nemotron Nano 9B v2","id":"nvidia-nemotron-nano-9b-v2","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-18","parameterCount":8900000000,"organization":{"id":"nvidia","name":"NVIDIA","website":"https://nvidia.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks."},{"name":"Llama 3.1 Nemotron Ultra 253B v1","id":"llama-3.1-nemotron-ultra-253b-v1","tags":["text-to-text"],"knowledgeCutoff":"2023-12-01","releaseDate":"2025-04-07","parameterCount":253000000000,"organization":{"id":"nvidia","name":"NVIDIA","website":"https://nvidia.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context."},{"name":"Llama 3.1 Nemotron Nano 8B V1","id":"llama-3.1-nemotron-nano-8b-v1","tags":["text-to-text"],"knowledgeCutoff":"2023-12-31","releaseDate":"2025-03-18","parameterCount":8000000000,"organization":{"id":"nvidia","name":"NVIDIA","website":"https://nvidia.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling."},{"name":"Llama-3.3 Nemotron Super 49B v1","id":"llama-3.3-nemotron-super-49b-v1","tags":["text-to-text"],"knowledgeCutoff":"2023-12-31","releaseDate":"2025-03-18","parameterCount":49900000000,"organization":{"id":"nvidia","name":"NVIDIA","website":"https://nvidia.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO)."},{"name":"Llama 3.1 Nemotron 70B Instruct","id":"llama-3.1-nemotron-70b-instruct","tags":["text-to-text"],"knowledgeCutoff":"2023-12-01","releaseDate":"2024-10-01","parameterCount":70000000000,"organization":{"id":"nvidia","name":"NVIDIA","website":"https://nvidia.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts."},{"name":"GPT-5.1","id":"gpt-5.1-2025-11-13","tags":["image-text-to-text"],"knowledgeCutoff":"2024-09-30","releaseDate":"2025-11-13","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"The best model for coding and agentic tasks with configurable reasoning effort. GPT-5.1 is our flagship model for coding and agentic tasks with configurable reasoning and non-reasoning effort."},{"name":"GPT-5.1 Instant","id":"gpt-5.1-instant-2025-11-12","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-12","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"GPT-5.1 Instant is the next iteration of GPT-5, designed to be more conversational than earlier chat models. It features improved instruction following and an adaptive reasoning capability that lets it decide when to think before responding. GPT-5.1 Instant offers clearer and more engaging interactions, reducing jargon and enhancing user comprehension. It demonstrates significant improvements in coding tasks, making it a valuable tool for developers. GPT-5.1 Auto routes each query to the model best suited for it, so users typically don't need to choose a model manually."},{"name":"GPT-5.1 Thinking","id":"gpt-5.1-thinking-2025-11-12","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-12","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":["thinking"],"deprecated":false}],"description":"GPT-5.1 Thinking is the next iteration of GPT-5, designed with enhanced adaptive reasoning capabilities. Unlike GPT-5.1 Instant, GPT-5.1 Thinking adapts thinking time more precisely to each question, providing more thorough reasoning for complex queries. It offers improved conversational abilities with clearer and more engaging interactions, reducing jargon and enhancing user comprehension. GPT-5.1 Thinking demonstrates significant improvements in coding tasks and complex reasoning scenarios, making it valuable for developers and users who need deeper analysis."},{"name":"GPT-5 Codex","id":"gpt-5-codex-2025-09-15","tags":["text-to-text"],"knowledgeCutoff":"2024-09-30","releaseDate":"2025-09-15","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"GPT-5 Codex has been trained specifically for conducting code reviews and finding critical flaws. When reviewing, it navigates your codebase and analyzes code patterns to identify potential security vulnerabilities, performance issues, and bugs."},{"name":"GPT-5","id":"gpt-5-2025-08-07","tags":["image-text-to-text"],"knowledgeCutoff":"2024-09-30","releaseDate":"2025-08-07","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.25,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"GPT-5 is a flagship model from OpenAI designed for coding, reasoning, and agentic tasks across domains. It is optimized for coding and agentic tasks with higher reasoning capabilities and medium speed."},{"name":"GPT-5 mini","id":"gpt-5-mini-2025-08-07","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-30","releaseDate":"2025-08-07","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.25,"outputPerMillion":2.0,"tags":[],"deprecated":false}],"description":"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost."},{"name":"GPT-5 nano","id":"gpt-5-nano-2025-08-07","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-30","releaseDate":"2025-08-07","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.05,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"GPT-5 nano is the fastest, cheapest version of GPT-5 from OpenAI. It is well-suited for summarization and classification tasks with average reasoning capabilities and very fast speed."},{"name":"GPT OSS 120B","id":"gpt-oss-120b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-05","parameterCount":116800000000,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"deepinfra","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.09,"outputPerMillion":0.45,"tags":[],"deprecated":false},{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false}],"description":"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters."},{"name":"GPT OSS 20B","id":"gpt-oss-20b","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-05","parameterCount":20900000000,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.05,"outputPerMillion":0.2,"tags":[],"deprecated":false},{"id":"fireworks","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"groq","provider":{"id":"groq","name":"Groq","website":"https://groq.com/"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.1,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3â€‘mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPTâ€‘4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters."},{"name":"o3-pro","id":"o3-pro-2025-06-10","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-31","releaseDate":"2025-06-10","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":20.0,"outputPerMillion":80.0,"tags":[],"deprecated":false}],"description":"Version of o3 with more compute for better responses. The o3-pro model uses more compute to think harder and provide consistently better answers. Designed to tackle tough problems with advanced reasoning capabilities."},{"name":"o3","id":"o3-2025-04-16","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-31","releaseDate":"2025-04-16","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":2.0,"outputPerMillion":8.0,"tags":[],"deprecated":false}],"description":"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images."},{"name":"o4-mini","id":"o4-mini","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-31","releaseDate":"2025-04-16","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.1,"outputPerMillion":4.4,"tags":[],"deprecated":false}],"description":"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3."},{"name":"GPT-4.1","id":"gpt-4.1-2025-04-14","tags":["image-text-to-text"],"knowledgeCutoff":"2024-06-01","releaseDate":"2025-04-14","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":2.0,"outputPerMillion":8.0,"tags":[],"deprecated":false}],"description":"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness."},{"name":"GPT-4.1 mini","id":"gpt-4.1-mini-2025-04-14","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-31","releaseDate":"2025-04-14","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.4,"outputPerMillion":1.6,"tags":[],"deprecated":false}],"description":"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost."},{"name":"GPT-4.1 nano","id":"gpt-4.1-nano-2025-04-14","tags":["image-text-to-text"],"knowledgeCutoff":"2024-05-31","releaseDate":"2025-04-14","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.1,"outputPerMillion":0.4,"tags":[],"deprecated":false}],"description":"GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion."},{"name":"GPT-4.5","id":"gpt-4.5","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-02-27","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":75.0,"outputPerMillion":150.0,"tags":[],"deprecated":false}],"description":"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy."},{"name":"o3-mini","id":"o3-mini","tags":["text-to-text"],"knowledgeCutoff":"2023-09-30","releaseDate":"2025-01-30","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":1.1,"outputPerMillion":4.4,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":1.1,"outputPerMillion":4.4,"tags":[],"deprecated":false}],"description":"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks."},{"name":"o1","id":"o1-2024-12-17","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-12-17","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":15.0,"outputPerMillion":60.0,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":15.0,"outputPerMillion":60.0,"tags":[],"deprecated":false}],"description":"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities."},{"name":"o1-pro","id":"o1-pro","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2023-09-30","releaseDate":"2024-12-17","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"o1-pro is OpenAI's advanced language model optimized for complex reasoning and specialized professional tasks, offering enhanced capabilities while maintaining high efficiency."},{"name":"o1-mini","id":"o1-mini","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-12","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":3.0,"outputPerMillion":12.0,"tags":[],"deprecated":false},{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":3.3,"outputPerMillion":13.2,"tags":[],"deprecated":false}],"description":"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources."},{"name":"o1-preview","id":"o1-preview","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-09-12","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":15.0,"outputPerMillion":60.0,"tags":[],"deprecated":false},{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":16.5,"outputPerMillion":66.0,"tags":[],"deprecated":false}],"description":"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities."},{"name":"GPT-4o","id":"gpt-4o-2024-08-06","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-06","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":2.5,"outputPerMillion":10.0,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":2.5,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding."},{"name":"GPT-4o mini","id":"gpt-4o-mini-2024-07-18","tags":["image-text-to-text"],"knowledgeCutoff":"2023-10-01","releaseDate":"2024-07-18","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":0.15,"outputPerMillion":0.6,"tags":[],"deprecated":false}],"description":"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats."},{"name":"GPT-4o","id":"gpt-4o-2024-05-13","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-05-13","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":2.5,"outputPerMillion":10.0,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":2.5,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding."},{"name":"GPT-4 Turbo","id":"gpt-4-turbo-2024-04-09","tags":["text-to-text"],"knowledgeCutoff":"2023-12-31","releaseDate":"2024-04-09","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":10.0,"outputPerMillion":30.0,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":10.0,"outputPerMillion":30.0,"tags":[],"deprecated":false}],"description":"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions."},{"name":"GPT-4","id":"gpt-4-0613","tags":["image-text-to-text"],"knowledgeCutoff":"2022-12-31","releaseDate":"2023-06-13","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":true,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":30.0,"outputPerMillion":60.0,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":30.0,"outputPerMillion":60.0,"tags":[],"deprecated":false}],"description":"GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks."},{"name":"GPT-3.5 Turbo","id":"gpt-3.5-turbo-0125","tags":["text-to-text"],"knowledgeCutoff":"2021-09-30","releaseDate":"2023-03-21","parameterCount":0,"organization":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"multiModal":false,"modelProviders":[{"id":"azure","provider":{"id":"azure","name":"Azure","website":"https://azure.microsoft.com"},"inputPerMillion":0.5,"outputPerMillion":1.5,"tags":[],"deprecated":false},{"id":"openai","provider":{"id":"openai","name":"OpenAI","website":"https://openai.com"},"inputPerMillion":0.5,"outputPerMillion":1.5,"tags":[],"deprecated":false}],"description":"The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls."},{"name":"Grok-4 Heavy","id":"grok-4-heavy","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":"2024-12-31","releaseDate":null,"parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities."},{"name":"Grok-4.1","id":"grok-4.1-2025-11-17","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"grok-4.1-2025-11-17","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Grok 4.1 (Non-Thinking) brings significant improvements to the real-world usability of Grok. The model is exceptionally capable in creative, emotional, and collaborative interactions. It is more perceptive to nuanced intent, compelling to speak with, and coherent in personality, while fully retaining the razor-sharp intelligence and reliability of its predecessors. Grok 4.1 uses no thinking tokens for immediate responses, making it faster while maintaining high quality. The model features reduced hallucinations compared to previous versions, with significant improvements in factual accuracy for information-seeking prompts. To achieve this, xAI used large scale reinforcement learning infrastructure to optimize style, personality, helpfulness, and alignment, developing new methods that use frontier agentic reasoning models as reward models to autonomously evaluate and iterate on responses at scale. Grok 4.1 includes comprehensive safety mitigations including refusal training, input filters for restricted knowledge, and adversarial robustness measures. The model demonstrates strong refusal rates on harmful queries (5% answer rate on chat refusals, 4% on agentic refusals) and improved honesty training to reduce deception."},{"name":"Grok-4.1 Fast Non-Reasoning","id":"grok-4-1-fast-non-reasoning","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"Bringing the next generation of tool-calling agents to the xAI API. Grok-4.1 Fast Non-Reasoning uses no thinking tokens for immediate responses, making it faster while maintaining high quality."},{"name":"Grok-4.1 Fast Reasoning","id":"grok-4-1-fast-reasoning","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"Bringing the next generation of tool-calling agents to the xAI API. Grok-4.1 Fast Reasoning is a high-speed variant optimized for faster inference while maintaining strong reasoning capabilities through thinking tokens."},{"name":"Grok-4.1 Thinking","id":"grok-4.1-thinking-2025-11-17","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-11-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":["thinking"],"deprecated":false}],"description":"Grok 4.1 Thinking (code name: quasarflux) is the reasoning variant of Grok 4.1, bringing significant improvements to the real-world usability of Grok. The model is exceptionally capable in creative, emotional, and collaborative interactions. It is more perceptive to nuanced intent, compelling to speak with, and coherent in personality, while fully retaining the razor-sharp intelligence and reliability of its predecessors. Grok 4.1 Thinking uses thinking tokens for deeper reasoning, achieving state-of-the-art performance on blind human preference evaluations. The model features reduced hallucinations compared to previous versions, with significant improvements in factual accuracy for information-seeking prompts. To achieve this, xAI used large scale reinforcement learning infrastructure to optimize style, personality, helpfulness, and alignment, developing new methods that use frontier agentic reasoning models as reward models to autonomously evaluate and iterate on responses at scale. Grok 4.1 Thinking includes comprehensive safety mitigations including refusal training, input filters for restricted knowledge, and adversarial robustness measures. The model demonstrates strong refusal rates on harmful queries (7% answer rate on chat refusals, 14% on agentic refusals) and improved honesty training to reduce deception."},{"name":"Grok 4 Fast","id":"grok-4-fast","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-28","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"Grok 4 Fast is a high-speed variant of Grok-4, optimized for faster inference while maintaining strong reasoning capabilities. It offers improved throughput and lower latency compared to the standard Grok-4 model."},{"name":"Grok-4 Fast Non-Reasoning","id":"grok-4-fast-non-reasoning","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-28","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"grok-4-fast-non-reasoning","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"Pushing the Frontier of Cost-Efficient Intelligence. Grok-4 Fast Non-Reasoning uses no thinking tokens for immediate responses, making it faster while maintaining high quality."},{"name":"Grok-4 Fast Reasoning","id":"grok-4-fast-reasoning","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-28","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":0.5,"tags":["thinking"],"deprecated":false}],"description":"Pushing the Frontier of Cost-Efficient Intelligence. Grok-4 Fast Reasoning is a high-speed variant of Grok-4 optimized for faster inference while maintaining strong reasoning capabilities through thinking tokens."},{"name":"Grok Code Fast 1","id":"grok-code-fast-1","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-28","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":false,"modelProviders":[{"id":"grok-code-fast-1","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.2,"outputPerMillion":1.5,"tags":[],"deprecated":false}],"description":"Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. Built from scratch with a brand-new model architecture, it features a pre-training corpus rich with programming-related content and post-training datasets that reflect real-world pull requests and coding tasks. The model has mastered the use of common tools like grep, terminal, and file editing, making it ideal for integration with IDEs. It is exceptionally versatile across the full software development stack and is particularly adept at TypeScript, Python, Java, Rust, C++, and Go."},{"name":"Grok-4","id":"grok-4","tags":["image-text-to-text"],"knowledgeCutoff":"2024-12-31","releaseDate":"2025-07-09","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version."},{"name":"Grok-3","id":"grok-3","tags":["image-text-to-text"],"knowledgeCutoff":"2024-11-17","releaseDate":"2025-02-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"grok-3","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":3.0,"outputPerMillion":15.0,"tags":[],"deprecated":false}],"description":"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science."},{"name":"Grok-3 Mini","id":"grok-3-mini","tags":["image-text-to-text"],"knowledgeCutoff":"2024-11-17","releaseDate":"2025-02-17","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":0.3,"outputPerMillion":0.5,"tags":[],"deprecated":false}],"description":"Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use."},{"name":"Grok-2 Image 1212","id":"grok-2-image-1212","tags":["text-to-image"],"knowledgeCutoff":null,"releaseDate":"2024-12-12","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":false,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":null,"outputPerMillion":null,"tags":[],"deprecated":false}],"description":"Our latest image generation model capable of generating multiple images from text prompt."},{"name":"Grok-2","id":"grok-2","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-13","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"xai","provider":{"id":"xai","name":"xAI","website":"https://docs.x.ai"},"inputPerMillion":2.0,"outputPerMillion":10.0,"tags":[],"deprecated":false}],"description":"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science."},{"name":"Grok-2 mini","id":"grok-2-mini","tags":["audio-video-image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-08-13","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions."},{"name":"Grok-1.5V","id":"grok-1.5v","tags":["image-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-04-12","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":true,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities."},{"name":"Grok-1.5","id":"grok-1.5","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2024-03-28","parameterCount":0,"organization":{"id":"xai","name":"xAI","website":"https://x.ai"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor."},{"name":"GLM-4.6","id":"glm-4.6","tags":["video-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-09-30","parameterCount":357000000000,"organization":{"id":"zai-org","name":"Zhipu AI","website":"https://z.ai"},"multiModal":true,"modelProviders":[{"id":"deepinfra-glm-4.6","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.6,"outputPerMillion":2.0,"tags":[],"deprecated":false}],"description":"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks."},{"name":"GLM-4.5V","id":"glm-4.5v","tags":["video-text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-08-11","parameterCount":108000000000,"organization":{"id":"zai-org","name":"Zhipu AI","website":"https://z.ai"},"multiModal":true,"modelProviders":[{"id":"novita","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.6,"outputPerMillion":2.2,"tags":[],"deprecated":false}],"description":"GLM-4.5V is a multimodal (vision-language) model based on GLM-4.5-Air (106B total, 12B active) that extends hybrid reasoning to images and video. It achieves state-of-the-art results across 40+ VLM benchmarks (image reasoning, video understanding, GUI tasks, chart/document parsing, grounding) while supporting a Thinking Mode switch for deep reasoning. Released under MIT with FP8/BF16 variants and tooling in Transformers, vLLM, and SGLang."},{"name":"GLM-4.5","id":"glm-4.5","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-28","parameterCount":355000000000,"organization":{"id":"zai-org","name":"Zhipu AI","website":"https://z.ai"},"multiModal":false,"modelProviders":[{"id":"deepinfra-glm-4.5","provider":{"id":"deepinfra","name":"DeepInfra","website":"https://deepinfra.com/"},"inputPerMillion":0.4,"outputPerMillion":1.6,"tags":[],"deprecated":false},{"id":"fireworks-glm-4.5","provider":{"id":"fireworks","name":"Fireworks","website":"https://fireworks.ai/"},"inputPerMillion":0.4,"outputPerMillion":1.6,"tags":[],"deprecated":false},{"id":"novita-glm-4.5","provider":{"id":"novita","name":"Novita","website":"https://novita.ai/"},"inputPerMillion":0.6,"outputPerMillion":2.2,"tags":[],"deprecated":false}],"description":"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development."},{"name":"GLM-4.5-Air","id":"glm-4.5-air","tags":["text-to-text"],"knowledgeCutoff":null,"releaseDate":"2025-07-28","parameterCount":106000000000,"organization":{"id":"zai-org","name":"Zhipu AI","website":"https://z.ai"},"multiModal":false,"modelProviders":[{"id":"no-id","provider":{"id":"no-id","name":"no-provider","website":"no-website"},"inputPerMillion":0.0,"outputPerMillion":0.0,"tags":["no-tags"],"deprecated":false}],"description":"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use."}]